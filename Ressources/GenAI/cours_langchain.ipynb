{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cours Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangChain** est un framework conçu pour faciliter la création d'applications basées sur des modèles de langage, comme les chatbots, les assistants virtuels (agents) ou les outils RAG (Retrieval Augmented Generation).\n",
    "\n",
    "Son utilité principale est donc de simplifier le développement d'applications qui nécessitent l'intégration et la gestion de plusieurs étapes dans le traitement du langage naturel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principe des chaines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain permet de composer des **chaînes de composants**, comme les `prompts`, les `LLM`, et les `parsers`, afin de traiter efficacement les entrées, générer des réponses et extraire des informations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"media/Chaine_Langchain.png\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans une chaîne, chaque étape est un objet `Runnable` (plus de détails plus loin).\n",
    "\n",
    "Chaque étape est exécutée dans l'ordre dans lequel elles sont ajoutées à la chaîne, et **la sortie de l'étape précédente est passée en entrée de l'étape suivante**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les composants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"media/Composants_Langchain.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous ces composants sont des [Runnable](https://python.langchain.com/v0.1/docs/expression_language/interface/). Un Runnable est un objet qui peut être appelé (avec Invoke, Batch ou Stream) pour exécuter une tâche. Ces runnables peuvent être enchaînés pour créer des chaines de traitement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple de chaine de composants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un exemple de chaine basique, écrit avec le LangChain Expression Language (LCEL) :\n",
    "\n",
    "```python\n",
    "\n",
    "from langchain_core.output_parsers.string import StringOutputParser\n",
    "\n",
    "chain = prompt | model | StringOutputParser()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Voyons chacun de ces éléments un à un"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Documentation](https://python.langchain.com/docs/concepts/#prompt-templates)*\n",
    "\n",
    "Bien souvent le premier élément d'une chaîne. \n",
    "\n",
    "On va souvent créer un template de prompt puis utiliser [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) pour créer un prompt avec des instructions et des variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis lors de l'invocation, on va lui donner un dictionnaire avec les `input_variables`, et il va les remplacer dans le template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\", additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = prompt_template.invoke(\n",
    "    {\n",
    "        \"style\": \"American English in a calm and respectful tone\",\n",
    "        \"text\": \"\"\"\n",
    "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help right now, matey!\n",
    "\"\"\"\n",
    "    }\n",
    ")\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs & Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On différencie `LLMs` classiques et les `Chat Models` par le fait que le Chat Model utilise une séquence de messages en input et renvoie des ChatMessage en output. UN LLM classique renvoie juste une string.\n",
    "\n",
    "La plupart du temps on va utiliser des `Chat Models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001DD67B69A90>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001DD67B6B6E0>, root_client=<openai.OpenAI object at 0x000001DD67B0A7E0>, root_async_client=<openai.AsyncOpenAI object at 0x000001DD67B69AC0>, temperature=0.1, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Documentation](https://python.langchain.com/docs/how_to/#output-parsers)*\n",
    "\n",
    "Les parsers ou output parsers sont utilisés pour transformer les outputs de LLM en format plus adapté. Utilisez les parsers pour récuperer des données en format plus structuré (json, str...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n  \"fruits\": [\\n    {\\n      \"name\": \"apple\",\\n      \"color\": \"red\",\\n      \"taste\": \"sweet\"\\n    },\\n    {\\n      \"name\": \"banana\",\\n      \"color\": \"yellow\",\\n      \"taste\": \"creamy\"\\n    },\\n    {\\n      \"name\": \"orange\",\\n      \"color\": \"orange\",\\n      \"taste\": \"citrusy\"\\n    },\\n    {\\n      \"name\": \"strawberry\",\\n      \"color\": \"red\",\\n      \"taste\": \"juicy\"\\n    },\\n    {\\n      \"name\": \"kiwi\",\\n      \"color\": \"green\",\\n      \"taste\": \"tangy\"\\n    }\\n  ]\\n}', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 149, 'prompt_tokens': 15, 'total_tokens': 164, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1741e867-61ca-461b-a6b8-808920ab8c8e-0', usage_metadata={'input_tokens': 15, 'output_tokens': 149, 'total_tokens': 164})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple simple :\n",
    "import json\n",
    "\n",
    "response = model.invoke(\"give me a json with 5 fruits\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on a une réponse de type `AIMessage`, et le contenu de la réponse est une string de json, qui n'aura même pas forcément toujours le même format, les mêmes clés, ou les mêmes valeurs. Mais imaginons qu'on veule récupérer le résultat dans un dictionnaire python avec un format spécifique pour pouvoir récupérer les informations ensuite.\n",
    "\n",
    "On peut alors utiliser `StructuredOutputParser`\n",
    "\n",
    "Note : Il y a plusieurs façons de forcer le format JSON en output, mais la façon la plus propre est en utilisant `Pydantic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fruits': [{'name': 'Apple', 'color': 'Red', 'taste': 'Sweet'},\n",
       "  {'name': 'Banana', 'color': 'Yellow', 'taste': 'Sweet'},\n",
       "  {'name': 'Orange', 'color': 'Orange', 'taste': 'Citrusy'},\n",
       "  {'name': 'Grapes', 'color': 'Purple', 'taste': 'Sweet'},\n",
       "  {'name': 'Lemon', 'color': 'Yellow', 'taste': 'Sour'}]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Fruit(BaseModel):\n",
    "    name: str = Field(description=\"name of the fruit\")\n",
    "    color: str = Field(description=\"color of the fruit\")\n",
    "    taste: str = Field(description=\"taste of the fruit, can be one of sweet, sour, bitter, tangy, or citrusy\")\n",
    "\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Fruit)\n",
    "\n",
    "fruit_query = \"Give me a list of 5 fruits\"\n",
    "\n",
    "# ici le parser va automatiquement créer des instructions à donner au modèle pour qu'il formate le résultat correctement\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()} \n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": fruit_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on regarde ce que `parser.get_format_instructions()` a fait : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"description\": \"name of the fruit\", \"title\": \"Name\", \"type\": \"string\"}, \"color\": {\"description\": \"color of the fruit\", \"title\": \"Color\", \"type\": \"string\"}, \"taste\": {\"description\": \"taste of the fruit, can be one of sweet, sour, bitter, tangy, or citrusy\", \"title\": \"Taste\", \"type\": \"string\"}}, \"required\": [\"name\", \"color\", \"taste\"]}\\n```'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[documentation](https://python.langchain.com/docs/how_to/#retrievers)*\n",
    "\n",
    "Les `Retrievers` sont chargés de prendre une requête et de renvoyer des documents pertinents.\n",
    "\n",
    "Ils sont principalement utilisés dans le cas de [Retrieval Augmented Generation](https://python.langchain.com/docs/tutorials/rag/) (RAG), où ils permettent de récupérer des documents pertinents à fournir à un LLM pour répondre à une question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Documentation](https://python.langchain.com/docs/how_to/#tools)*\n",
    "\n",
    "Les `Tools` sont des utilitaires conçus pour être appelés par un modèle : leurs inputs sont conçues pour être générées par les modèles et leurs outputs sont conçues pour être renvoyées aux modèles. Les `Tools` sont nécessaires chaque fois que vous souhaitez qu'un modèle contrôle des parties de votre code ou fasse appel à des API externes.\n",
    "\n",
    "La suite sur les Agents et tools plus loin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents & Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GENAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
