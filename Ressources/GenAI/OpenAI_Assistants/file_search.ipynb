{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Search\n",
    "\n",
    "https://platform.openai.com/docs/assistants/tools/file-search?context=without-streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI, OpenAI\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_api_endpoint = os.getenv(\"AZURE_OPENAI_API_ENDPOINT\")\n",
    "deployment_name = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "openai_api_key = os.getenv('OPENAI_SIMPLON_API_KEY')\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we’ll create an assistant that can help answer questions about Certif IA 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a new Assistant with File Search Enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new assistant with `file_search` enabled in the `tools` parameter of the Assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Assistant Certification IA 2023\",\n",
    "  instructions=\"Tu réponds uniquement aux questions concernant la certification IA 2023 délivrée par Simplon.\",\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the `file_search` tool is enabled, the model decides when to retrieve content based on user messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Upload files and add them to a Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access your files, the `file_search` tool uses the Vector Store object. Upload your files and create a Vector Store to contain them. Once the Vector Store is created, you should poll its status until all files are out of the `in_progress` state to ensure that all content has finished processing. The SDK provides helpers to uploading and polling in one shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant(id='asst_spGGvyg1alU3tvQBU3PWw0Gg', created_at=1721901675, description=None, instructions='Tu réponds uniquement aux questions concernant la certification IA 2023 délivrée par Simplon.', metadata={}, model='gpt-3.5-turbo', name='Assistant Certification IA 2023', object='assistant', tools=[FileSearchTool(type='file_search')], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=ToolResourcesFileSearch(vector_store_ids=[])), top_p=1.0)\n"
     ]
    }
   ],
   "source": [
    "print(assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"vsfb_e587a6bc697f4ef0b6ee72d52fc80f11\",\n",
      "  \"created_at\": 1721908713,\n",
      "  \"file_counts\": {\n",
      "    \"cancelled\": 0,\n",
      "    \"completed\": 1,\n",
      "    \"failed\": 0,\n",
      "    \"in_progress\": 0,\n",
      "    \"total\": 1\n",
      "  },\n",
      "  \"object\": \"vector_store.file_batch\",\n",
      "  \"status\": \"completed\",\n",
      "  \"vector_store_id\": \"vs_vUsPdfeq1ymnt0mC3BuU39Bq\"\n",
      "}\n",
      "completed\n",
      "FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)\n"
     ]
    }
   ],
   "source": [
    "# bout de code pour éviter de créer plein de fois le même vector srore\n",
    "try : \n",
    "  vector_store = client.beta.vector_stores.retrieve(\n",
    "    vector_store_id=\"vs_vUsPdfeq1ymnt0mC3BuU39Bq\"\n",
    "  )\n",
    "  print(vector_store)\n",
    "  print(\"vector store already exists\")\n",
    "\n",
    "except Exception as e:\n",
    "  print(\"vector store not found\")\n",
    "\n",
    "  # Create a vector store caled \"Full Certif IA 2023\"\n",
    "  vector_store = client.beta.vector_stores.create(name=\"Full Certif IA 2023\")\n",
    "  \n",
    "  # Ready the files for upload to OpenAI\n",
    "  file_paths = [\"reglement_specifique_full_dev_ia_2023.pdf\"]\n",
    "  file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "  \n",
    "  # Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "  # and poll the status of the file batch for completion.\n",
    "  file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id, files=file_streams\n",
    "  )\n",
    "  \n",
    "  # You can print the status and the file counts of the batch to see the result of this operation.\n",
    "  print(file_batch.model_dump_json(indent=2))\n",
    "  print(file_batch.status)\n",
    "  print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Update the assistant to to use the new Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the files accessible to your assistant, update the assistant’s `tool_resources` with the new `vector_store` id.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorStore(id='vs_mNwTx0v5cZoZWP2Ze21hQWfY', created_at=1721910340, file_counts=FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1), last_active_at=1721910340, metadata={}, name=None, object='vector_store', status='completed', usage_bytes=3040, expires_after=ExpiresAfter(anchor='last_active_at', days=7), expires_at=1722515140)\n",
      "vector store already exists\n"
     ]
    }
   ],
   "source": [
    "try : \n",
    "  vector_store = client.beta.vector_stores.retrieve(\n",
    "    vector_store_id=\"vs_mNwTx0v5cZoZWP2Ze21hQWfY\"\n",
    "  )\n",
    "  print(vector_store)\n",
    "  print(\"vector store already exists\")\n",
    "\n",
    "except Exception as e:\n",
    "  print(\"vector store not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create a thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also attach files as Message attachments on your thread. Doing so will create another `vector_store` associated with the thread, or, if there is already a vector store attached to this thread, attach the new files to the existing thread vector store. When you create a Run on this thread, the file search tool will query both the `vector_store` from your assistant and the `vector_store` on the thread.\n",
    "\n",
    "In this example, the user attached a copy of the résumé certif IA 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolResourcesFileSearch(vector_store_ids=['vs_mNwTx0v5cZoZWP2Ze21hQWfY'])\n"
     ]
    }
   ],
   "source": [
    "# encore une fois bloc pour pas upload 100 fois le même file\n",
    "try : \n",
    "  vector_store = client.beta.vector_stores.retrieve(\n",
    "    vector_store_id=\"vs_mNwTx0v5cZoZWP2Ze21hQWfY\"\n",
    "  )\n",
    "  print(vector_store)\n",
    "  print(\"vector store already exists\")\n",
    "\n",
    "except Exception as e:\n",
    "  print(\"vector store not found\")\n",
    "\n",
    "  # Upload the user provided file to OpenAI\n",
    "  message_file = client.files.create(\n",
    "    file=open(\"resume_certif_ia_2023.pdf\", \"rb\"), purpose=\"assistants\"\n",
    "  )\n",
    "\n",
    "  message_file_id = \"file-QEZAdgUjplVNJdIW24ZAv28b\" # je le remets ici pour éviter de le reupload plein de fois\n",
    "\n",
    "  # Create a thread and attach the file to the message\n",
    "  thread = client.beta.threads.create(\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Sur quoi porte le bloc de compétences 1 ?\",\n",
    "        # Attach the new file to the message.\n",
    "        \"attachments\": [\n",
    "          { \"file_id\": message_file_id, \"tools\": [{\"type\": \"file_search\"}]}\n",
    "        ],\n",
    "      }\n",
    "    ]\n",
    "  )\n",
    "  \n",
    "  # The thread now has a vector store with that file in its tool resources.\n",
    "  print(thread.tool_resources.file_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector stores created using message attachements have a default expiration policy of 7 days after they were last active (defined as the last time the vector store was part of a run). This default exists to help you manage your vector storage costs. You can override these expiration policies at any time. Learn more [here](https://platform.openai.com/docs/assistants/tools/file-search/managing-costs-with-expiration-policies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create a run and check the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a Run and observe that the model uses the File Search tool to provide a response to the user’s question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tu peux accéder à la certification IA 2023 délivrée par Simplon en étant en contrat d'apprentissage, comme le contrat de professionnalisation est l'une des voies d'accès possibles pour obtenir cette certification[0].\n",
      "[0] reglement_specifique_dev_ia_2023.pdf\n"
     ]
    }
   ],
   "source": [
    "# Use the create and poll SDK helper to create a run and poll the status of\n",
    "# the run until it's in a terminal state.\n",
    "\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id,\n",
    "    instructions=\"\"\"\n",
    "      Tu réponds uniquement aux questions concernant la certification IA 2023 délivrée par Simplon.\n",
    "      Si la réponse à la question n'est pas compris dans les fichiers, dis-le.\"\"\" \n",
    ")\n",
    "# je change les instructions de l'assistant ici mais c'est bien sûr optionnel\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "\n",
    "message_content = messages[0].content[0].text\n",
    "annotations = message_content.annotations\n",
    "citations = []\n",
    "for index, annotation in enumerate(annotations):\n",
    "    message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "    if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "        cited_file = client.files.retrieve(file_citation.file_id)\n",
    "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "print(message_content.value)\n",
    "print(\"\\n\".join(citations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que les annotations sont gérées pour être affichées correctement ici. Les annotations peuvent être des file_path ou des file_citation. Pour plus d'information sur comment les afficher correctement : https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/assistant#message-annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FileCitationAnnotation(end_index=366, file_citation=FileCitation(file_id='file-7WDWcUBr05dDTP6MLtWI9Dys', quote=None), start_index=354, text='【4:1†source】', type='file_citation')]\n"
     ]
    }
   ],
   "source": [
    "print(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A vous de jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Créez un vector store\n",
    "- Ajoutez y le fichier [reglement spécifique de 5 pages](https://github.com/louiskuhn/IA-P3-Euskadi/blob/main/Ressources/GenAI/OpenAI_Assistants/reglement_specifique_5_pages_dev_ia_2023.pdf)\n",
    "- Créez un assistant et associez lui le vector store créé précédemment\n",
    "- Testez l'assistant sur des questions les blocs de compétences, puis sur la composition du jury (qu'il ne connait pas normalement)\n",
    "- Supprimez votre vector store et modifiez votre assistant pour qu'il utilise maintenant le vector store que j'ai créé, avec le réglement complet de la certification, le vector_store_id est : `vs_vUsPdfeq1ymnt0mC3BuU39Bq`\n",
    "- Retestez les questions d'au-dessus\n",
    "- Supprimez votre assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GENAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
