{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Recurrent Neural Network - Analyse de sentiments**\n",
    "\n",
    "Le but de cet exercice est la classification binaire (positive ou négative) de reviews en utilisant un RNN sur un dataset de commentaires IMDB que vous trouverez dans le dossier data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Import des données et preprocessing**\n",
    "\n",
    "1. Importer les 2 fichiers \".txt\" de data/imdb\n",
    "2. Vérifier ce qu'il y a dedans si c'est pas déjà fait...\n",
    "3. Un peu de preprocessing de texte, allez on va chercher dans sa petite mémoire et sinon dans son gros ordinateur :\n",
    ">- convertir en minuscules\n",
    ">- retirer la ponctuation\n",
    ">- créer une liste des reviews et déterminer combien il y en a. Pareil pour les labels. Inch'Allah y en aura autant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import des txt\n",
    "with open('data/imdb/reviews.txt', 'r') as f:\n",
    "    reviews_str = f.read()\n",
    "with open('data/imdb/labels.txt', 'r') as f:\n",
    "    labels_str = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str,\n",
       " 33678267,\n",
       " 'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \\nstory of a man who has unnatural feelings for a pig . starts out wi')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(reviews_str), len(reviews_str), reviews_str[:900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str,\n",
       " 225000,\n",
       " 'positive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nn')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels_str), len(labels_str), labels_str[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t   \\nstory of a man who has unnatural feelings for a pig  starts out with a opening scene that is a'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nettoyage des string\n",
    "reviews_str = reviews_str.lower()\n",
    "reviews_str = reviews_str.translate(str.maketrans('', '', string.punctuation))\n",
    "reviews_str[:900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rvws = reviews_str.splitlines()\n",
    "lbls = labels_str.splitlines()\n",
    "len(rvws), len(lbls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Tokenisation ou encodage**\n",
    "\n",
    "Le but est de remplacer les mots des reviews par des entiers. Si vous voulez faire à votre sauce, vous y êtes encouragés !\n",
    "\n",
    "Sinon, les quelques étapes décrites ci-dessous vous permettront de le faire :\n",
    ">- compter l'ensemble des mots (vous pouvez utiliser `Counter` de la librairie `collections` qui est une des plus rapide dans ce domaine)\n",
    ">- les trier par ordre décroissant d'occurrences\n",
    ">- créer un dictionnaire `word_mapping` où les clés sont les mots et les valeurs l'entier associé (votre dico doit être `{'the': 1, 'and': 2, 'a': 3, 'of': 4,..., 'muppified': 74070, 'whelk': 74071, 'hued': 74072}`). C'est volontaire que ça commence à 1 car on utilisera le \"0\" comme caractère spécial (il servira pour \"remplir\" les reviews les plus courtes afin qu'elles aient toutes la même taille...)\n",
    ">- encoder les mots (c'est-à-dire les remplacer par l'entier qui les représente)\n",
    ">- encoder les labels (ça c'est fastoche, on se débrouille)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comptage des mots et tri par occurences\n",
    "words = reviews_str.split()\n",
    "countWords = Counter(words)\n",
    "sorted_words = sorted(countWords, key=countWords.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionnaire de correspondance mot:entier\n",
    "word_mapping = {w:i+1 for i,w in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodage des mots\n",
    "rvws_num = [list(map(lambda w: word_mapping[w], rvw.split())) for rvw in rvws]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2514"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taille max des reviews\n",
    "max([len(rvw) for rvw in rvws_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodage des labels\n",
    "lbls_num = [1*(lab==\"positive\") for lab in lbls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Longueur des séquences**\n",
    "\n",
    "Il s'agit ici d'analyser la longueur des reviews pour déterminer éventuellement des outliers et choisir ce qu'on en fait. On va aussi \"uniformiser\" la longueur des séquence.\n",
    "\n",
    "1. Avec la méthode de votre choix (graphique, stats desc...), étudier la longueur des reviews\n",
    "2. Déterminer s'il y a des outliers et ce que vous souhaitez en faire (si vous les décidez de les supprimer, attention de bien supprimer aussi les labels correspondants...)\n",
    "3. Ça a été évoqué un peu plus haut, on veut que nos reviews aient toutes la même taille pour faciliter l'entraînement du réseau. Par conséquent on va ajouter des 0 aux reviews les plus courtes et tronquer les reviews les plus longues (*padding/truncating*, si vous vous souvenez bien on a vu il y a peu le *zero-padding* dans un certain cas...bon ben c'est pareil).  \n",
    ">- définir une fonction `trunc_pad(review_list, length)` :\n",
    ">>- qui prend en paramètres la liste des reviews (chaque review étant encodée en une liste d'entiers) et une longueur donnée\n",
    ">>- et qui retourne un array 2D avec (en ligne) les reviews trop longues tronquées et des 0 à gauche pour les reviews trop courtes. Avant de vous lancer et pour vous assurer d'avoir compris, quelles dimensions doit avoir votre array en sortie ?\n",
    ">>- tester votre fonction avec une longueur fixée à 250 et afficher les 5 premières valeurs des 5 premières reviews. Vous devez obtenir ça:\n",
    " \n",
    "\\[[    0     0     0     0     0]  \n",
    "[    0     0     0     0     0]  \n",
    "[22382    42 46418    15   706]  \n",
    "[ 4505   505    15     3  3342]  \n",
    "[    0     0     0     0     0]\\]\n",
    "\n",
    ">- maintenant, que c'est fait, je peux vous le dire, il y a une fonction dans `keras.preprocessing` qui permet de le faire. La trouver et comparer les temps d'éxecution des 2 fonctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    25000.00000\n",
      "mean       240.80784\n",
      "std        179.01773\n",
      "min         10.00000\n",
      "5%          66.00000\n",
      "10%         94.00000\n",
      "25%        130.00000\n",
      "50%        179.00000\n",
      "75%        293.00000\n",
      "90%        471.00000\n",
      "95%        618.00000\n",
      "max       2514.00000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAGdCAYAAAAbudkLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAp3UlEQVR4nO3df1RUd37/8RfBcQQOzIoWhtlgwvawrhvcNCUJYtLVrgJ6Qtg9nlOzi501p1ZJjVqK1sbaNmNOA1m3Uc+BJjHWE23Qkj82bnM2loAnCVkO+GNZ6fqrbnriGt1lxDU4oNBhgvf7R77czTj+YjMzyMfn4xzOcT7zns987nsuycvPzHUSLMuyBAAAYKC7RnsBAAAAsULQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYa9xoL2A0XblyRb/5zW+UmpqqhISE0V4OAAC4BZZlqa+vTx6PR3fddeM9mzs66PzmN79Rdnb2aC8DAAD8Hs6cOaO77777hjV3dNBJTU2V9Fmj0tLSojJnKBRSU1OTiouL5XA4ojInItHn+KDP8UGf44M+x0c8+tzb26vs7Gz7/+M3ckcHneG3q9LS0qIadJKTk5WWlsYvUgzR5/igz/FBn+ODPsdHPPt8Kx874cPIAADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYaN9oLwMjd+8zbMZv7Vy88FrO5AQCIN3Z0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYIw46H3zwgR5//HF5PB4lJCToxz/+cdj9lmXJ5/PJ4/EoKSlJs2fP1rFjx8JqgsGgVq5cqcmTJyslJUVlZWU6e/ZsWE1PT4+8Xq9cLpdcLpe8Xq8uXrwYVvPxxx/r8ccfV0pKiiZPnqxVq1ZpcHBwpIcEAAAMNeKgc/nyZd1///2qq6u75v0bN27Upk2bVFdXp0OHDsntdquoqEh9fX12TWVlpfbs2aOGhga1trbq0qVLKi0t1dDQkF1TXl6uzs5ONTY2qrGxUZ2dnfJ6vfb9Q0NDeuyxx3T58mW1traqoaFBP/rRj7R69eqRHhIAADDUiL/Uc/78+Zo/f/4177MsS1u2bNH69eu1YMECSdLOnTuVmZmp3bt3q6KiQoFAQNu3b9frr7+uuXPnSpLq6+uVnZ2tffv2qaSkRCdOnFBjY6P279+vgoICSdK2bdtUWFiokydPaurUqWpqatLx48d15swZeTweSdKLL76oJ598Us8//7zS0tJ+r4YAAABzRPXby0+dOiW/36/i4mJ7zOl0atasWWpra1NFRYU6OjoUCoXCajwej/Ly8tTW1qaSkhK1t7fL5XLZIUeSZsyYIZfLpba2Nk2dOlXt7e3Ky8uzQ44klZSUKBgMqqOjQ3/6p38asb5gMKhgMGjf7u3tlSSFQiGFQqGo9GB4nmjNdy3ORCtmc8dy3dEUjz6DPscLfY4P+hwf8ejzSOaOatDx+/2SpMzMzLDxzMxMnT592q4ZP368Jk6cGFEz/Hi/36+MjIyI+TMyMsJqrn6eiRMnavz48XbN1WpqarRhw4aI8aamJiUnJ9/KId6y5ubmqM73eRsfjtnU2rt3b+wmj4FY9hm/Q5/jgz7HB32Oj1j2ub+//5Zroxp0hiUkJITdtiwrYuxqV9dcq/73qfm8devWqaqqyr7d29ur7OxsFRcXR+2trlAopObmZhUVFcnhcERlzqvl+d6JybySdNRXErO5oykefQZ9jhf6HB/0OT7i0efhd2RuRVSDjtvtlvTZbktWVpY93t3dbe++uN1uDQ4OqqenJ2xXp7u7WzNnzrRrzp07FzH/+fPnw+Y5cOBA2P09PT0KhUIROz3DnE6nnE5nxLjD4Yj6ixGLOYcFh24cGr+IsfbLH8s+43foc3zQ5/igz/ERyz6PZN6o/js6OTk5crvdYdtVg4ODamlpsUNMfn6+HA5HWE1XV5eOHj1q1xQWFioQCOjgwYN2zYEDBxQIBMJqjh49qq6uLrumqalJTqdT+fn50TwsAAAwRo14R+fSpUv63//9X/v2qVOn1NnZqfT0dE2ZMkWVlZWqrq5Wbm6ucnNzVV1dreTkZJWXl0uSXC6XlixZotWrV2vSpElKT0/XmjVrNH36dPsqrGnTpmnevHlaunSptm7dKklatmyZSktLNXXqVElScXGxvv71r8vr9eqHP/yhPvnkE61Zs0ZLly7liisAACDp9wg6P/vZz8KuaBr+zMvixYu1Y8cOrV27VgMDA1q+fLl6enpUUFCgpqYmpaam2o/ZvHmzxo0bp4ULF2pgYEBz5szRjh07lJiYaNfs2rVLq1atsq/OKisrC/u3exITE/X2229r+fLleuSRR5SUlKTy8nL9y7/8y8i7AAAAjDTioDN79mxZ1vUvb05ISJDP55PP57tuzYQJE1RbW6va2trr1qSnp6u+vv6Ga5kyZYp+8pOf3HTNAADgzsR3XQEAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjEXQAQAAxop60Pn000/1D//wD8rJyVFSUpK+8pWv6LnnntOVK1fsGsuy5PP55PF4lJSUpNmzZ+vYsWNh8wSDQa1cuVKTJ09WSkqKysrKdPbs2bCanp4eeb1euVwuuVwueb1eXbx4MdqHBAAAxqioB50f/OAHeuWVV1RXV6cTJ05o48aN+uEPf6ja2lq7ZuPGjdq0aZPq6up06NAhud1uFRUVqa+vz66prKzUnj171NDQoNbWVl26dEmlpaUaGhqya8rLy9XZ2anGxkY1Njaqs7NTXq832ocEAADGqHHRnrC9vV3f/va39dhjj0mS7r33Xv3Hf/yHfvazn0n6bDdny5YtWr9+vRYsWCBJ2rlzpzIzM7V7925VVFQoEAho+/btev311zV37lxJUn19vbKzs7Vv3z6VlJToxIkTamxs1P79+1VQUCBJ2rZtmwoLC3Xy5ElNnTo12ocGAADGmKgHnUcffVSvvPKKfvnLX+qrX/2q/vu//1utra3asmWLJOnUqVPy+/0qLi62H+N0OjVr1iy1tbWpoqJCHR0dCoVCYTUej0d5eXlqa2tTSUmJ2tvb5XK57JAjSTNmzJDL5VJbW9s1g04wGFQwGLRv9/b2SpJCoZBCoVBUjn94nmjNdy3ORCtmc8dy3dEUjz6DPscLfY4P+hwf8ejzSOaOetD5u7/7OwUCAX3ta19TYmKihoaG9Pzzz+t73/ueJMnv90uSMjMzwx6XmZmp06dP2zXjx4/XxIkTI2qGH+/3+5WRkRHx/BkZGXbN1WpqarRhw4aI8aamJiUnJ4/wSG+subk5qvN93saHYza19u7dG7vJYyCWfcbv0Of4oM/xQZ/jI5Z97u/vv+XaqAedN954Q/X19dq9e7fuu+8+dXZ2qrKyUh6PR4sXL7brEhISwh5nWVbE2NWurrlW/Y3mWbdunaqqquzbvb29ys7OVnFxsdLS0m7p+G4mFAqpublZRUVFcjgcUZnzanm+d2IyryQd9ZXEbO5oikefQZ/jhT7HB32Oj3j0efgdmVsR9aDzt3/7t3rmmWf03e9+V5I0ffp0nT59WjU1NVq8eLHcbrekz3ZksrKy7Md1d3fbuzxut1uDg4Pq6ekJ29Xp7u7WzJkz7Zpz585FPP/58+cjdouGOZ1OOZ3OiHGHwxH1FyMWcw4LDt04EH4RY+2XP5Z9xu/Q5/igz/FBn+Mjln0eybxRv+qqv79fd90VPm1iYqJ9eXlOTo7cbnfYltbg4KBaWlrsEJOfny+HwxFW09XVpaNHj9o1hYWFCgQCOnjwoF1z4MABBQIBuwYAANzZor6j8/jjj+v555/XlClTdN999+nw4cPatGmT/uIv/kLSZ283VVZWqrq6Wrm5ucrNzVV1dbWSk5NVXl4uSXK5XFqyZIlWr16tSZMmKT09XWvWrNH06dPtq7CmTZumefPmaenSpdq6daskadmyZSotLeWKKwAAICkGQae2tlb/+I//qOXLl6u7u1sej0cVFRX6p3/6J7tm7dq1GhgY0PLly9XT06OCggI1NTUpNTXVrtm8ebPGjRunhQsXamBgQHPmzNGOHTuUmJho1+zatUurVq2yr84qKytTXV1dtA8JAACMUVEPOqmpqdqyZYt9Ofm1JCQkyOfzyefzXbdmwoQJqq2tDfuHBq+Wnp6u+vr6L7BaAABgMr7rCgAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYKxxo70Ak937zNujvQQAAO5o7OgAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGPFJOj8+te/1p//+Z9r0qRJSk5O1h/90R+po6PDvt+yLPl8Pnk8HiUlJWn27Nk6duxY2BzBYFArV67U5MmTlZKSorKyMp09ezaspqenR16vVy6XSy6XS16vVxcvXozFIQEAgDEo6kGnp6dHjzzyiBwOh/7rv/5Lx48f14svvqgvfelLds3GjRu1adMm1dXV6dChQ3K73SoqKlJfX59dU1lZqT179qihoUGtra26dOmSSktLNTQ0ZNeUl5ers7NTjY2NamxsVGdnp7xeb7QPCQAAjFHjoj3hD37wA2VnZ+u1116zx+699177z5ZlacuWLVq/fr0WLFggSdq5c6cyMzO1e/duVVRUKBAIaPv27Xr99dc1d+5cSVJ9fb2ys7O1b98+lZSU6MSJE2psbNT+/ftVUFAgSdq2bZsKCwt18uRJTZ06NdqHBgAAxpioB5233npLJSUl+rM/+zO1tLToy1/+spYvX66lS5dKkk6dOiW/36/i4mL7MU6nU7NmzVJbW5sqKirU0dGhUCgUVuPxeJSXl6e2tjaVlJSovb1dLpfLDjmSNGPGDLlcLrW1tV0z6ASDQQWDQft2b2+vJCkUCikUCkXl+IfnCYVCciZaUZkznqLVh1j7fJ8RO/Q5PuhzfNDn+IhHn0cyd9SDzkcffaSXX35ZVVVV+vu//3sdPHhQq1atktPp1Pe//335/X5JUmZmZtjjMjMzdfr0aUmS3+/X+PHjNXHixIia4cf7/X5lZGREPH9GRoZdc7Wamhpt2LAhYrypqUnJyckjP9gbaG5u1saHozplXOzdu3e0lzAizc3No72EOwJ9jg/6HB/0OT5i2ef+/v5bro160Lly5YoefPBBVVdXS5IeeOABHTt2TC+//LK+//3v23UJCQlhj7MsK2LsalfXXKv+RvOsW7dOVVVV9u3e3l5lZ2eruLhYaWlpNz+4WxAKhdTc3KyioiI98Py7UZkzno76SkZ7Cbfk8312OByjvRxj0ef4oM/xQZ/jIx59Hn5H5lZEPehkZWXp61//etjYtGnT9KMf/UiS5Ha7JX22I5OVlWXXdHd327s8brdbg4OD6unpCdvV6e7u1syZM+2ac+fORTz/+fPnI3aLhjmdTjmdzohxh8MR9RfD4XAoOHTj4HY7Gmu//LF47RCJPscHfY4P+hwfsezzSOaN+lVXjzzyiE6ePBk29stf/lL33HOPJCknJ0dutztsS2twcFAtLS12iMnPz5fD4Qir6erq0tGjR+2awsJCBQIBHTx40K45cOCAAoGAXQMAAO5sUd/R+Zu/+RvNnDlT1dXVWrhwoQ4ePKhXX31Vr776qqTP3m6qrKxUdXW1cnNzlZubq+rqaiUnJ6u8vFyS5HK5tGTJEq1evVqTJk1Senq61qxZo+nTp9tXYU2bNk3z5s3T0qVLtXXrVknSsmXLVFpayhVXAABAUgyCzkMPPaQ9e/Zo3bp1eu6555STk6MtW7Zo0aJFds3atWs1MDCg5cuXq6enRwUFBWpqalJqaqpds3nzZo0bN04LFy7UwMCA5syZox07digxMdGu2bVrl1atWmVfnVVWVqa6urpoHxIAABijoh50JKm0tFSlpaXXvT8hIUE+n08+n++6NRMmTFBtba1qa2uvW5Oenq76+vovslQAAGAwvusKAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADBWzINOTU2NEhISVFlZaY9ZliWfzyePx6OkpCTNnj1bx44dC3tcMBjUypUrNXnyZKWkpKisrExnz54Nq+np6ZHX65XL5ZLL5ZLX69XFixdjfUgAAGCMiGnQOXTokF599VV94xvfCBvfuHGjNm3apLq6Oh06dEhut1tFRUXq6+uzayorK7Vnzx41NDSotbVVly5dUmlpqYaGhuya8vJydXZ2qrGxUY2Njers7JTX643lIQEAgDEkZkHn0qVLWrRokbZt26aJEyfa45ZlacuWLVq/fr0WLFigvLw87dy5U/39/dq9e7ckKRAIaPv27XrxxRc1d+5cPfDAA6qvr9eRI0e0b98+SdKJEyfU2Niof/u3f1NhYaEKCwu1bds2/eQnP9HJkydjdVgAAGAMGReriZ9++mk99thjmjt3rv75n//ZHj916pT8fr+Ki4vtMafTqVmzZqmtrU0VFRXq6OhQKBQKq/F4PMrLy1NbW5tKSkrU3t4ul8ulgoICu2bGjBlyuVxqa2vT1KlTI9YUDAYVDAbt2729vZKkUCikUCgUleMenicUCsmZaEVlzniKVh9i7fN9RuzQ5/igz/FBn+MjHn0eydwxCToNDQ36+c9/rkOHDkXc5/f7JUmZmZlh45mZmTp9+rRdM378+LCdoOGa4cf7/X5lZGREzJ+RkWHXXK2mpkYbNmyIGG9qalJycvItHNmta25u1saHozplXOzdu3e0lzAizc3No72EOwJ9jg/6HB/0OT5i2ef+/v5bro160Dlz5oz++q//Wk1NTZowYcJ16xISEsJuW5YVMXa1q2uuVX+jedatW6eqqir7dm9vr7Kzs1VcXKy0tLQbPvetCoVCam5uVlFRkR54/t2ozBlPR30lo72EW/L5PjscjtFejrHoc3zQ5/igz/ERjz4PvyNzK6IedDo6OtTd3a38/Hx7bGhoSB988IHq6ursz8/4/X5lZWXZNd3d3fYuj9vt1uDgoHp6esJ2dbq7uzVz5ky75ty5cxHPf/78+YjdomFOp1NOpzNi3OFwRP3FcDgcCg7dOLjdjsbaL38sXjtEos/xQZ/jgz7HRyz7PJJ5o/5h5Dlz5ujIkSPq7Oy0fx588EEtWrRInZ2d+spXviK32x22pTU4OKiWlhY7xOTn58vhcITVdHV16ejRo3ZNYWGhAoGADh48aNccOHBAgUDArgEAAHe2qO/opKamKi8vL2wsJSVFkyZNsscrKytVXV2t3Nxc5ebmqrq6WsnJySovL5ckuVwuLVmyRKtXr9akSZOUnp6uNWvWaPr06Zo7d64kadq0aZo3b56WLl2qrVu3SpKWLVum0tLSa34QGQAA3HlidtXVjaxdu1YDAwNavny5enp6VFBQoKamJqWmpto1mzdv1rhx47Rw4UINDAxozpw52rFjhxITE+2aXbt2adWqVfbVWWVlZaqrq4v78QAAgNtTXILO+++/H3Y7ISFBPp9PPp/vuo+ZMGGCamtrVVtbe92a9PR01dfXR2mVAADANHzXFQAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAY43K5eW4fd37zNsxmfdXLzwWk3kBALgRdnQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGCsqAedmpoaPfTQQ0pNTVVGRoa+853v6OTJk2E1lmXJ5/PJ4/EoKSlJs2fP1rFjx8JqgsGgVq5cqcmTJyslJUVlZWU6e/ZsWE1PT4+8Xq9cLpdcLpe8Xq8uXrwY7UMCAABjVNSDTktLi55++mnt379fzc3N+vTTT1VcXKzLly/bNRs3btSmTZtUV1enQ4cOye12q6ioSH19fXZNZWWl9uzZo4aGBrW2turSpUsqLS3V0NCQXVNeXq7Ozk41NjaqsbFRnZ2d8nq90T4kAAAwRo2L9oSNjY1ht1977TVlZGSoo6ND3/zmN2VZlrZs2aL169drwYIFkqSdO3cqMzNTu3fvVkVFhQKBgLZv367XX39dc+fOlSTV19crOztb+/btU0lJiU6cOKHGxkbt379fBQUFkqRt27apsLBQJ0+e1NSpU6N9aAAAYIyJetC5WiAQkCSlp6dLkk6dOiW/36/i4mK7xul0atasWWpra1NFRYU6OjoUCoXCajwej/Ly8tTW1qaSkhK1t7fL5XLZIUeSZsyYIZfLpba2tmsGnWAwqGAwaN/u7e2VJIVCIYVCoagc7/A8oVBIzkQrKnOaIFr9vXq+aM+LcPQ5PuhzfNDn+IhHn0cyd0yDjmVZqqqq0qOPPqq8vDxJkt/vlyRlZmaG1WZmZur06dN2zfjx4zVx4sSImuHH+/1+ZWRkRDxnRkaGXXO1mpoabdiwIWK8qalJycnJIzy6G2tubtbGh6M65Zi2d+/emMzb3Nwck3kRjj7HB32OD/ocH7Hsc39//y3XxjTorFixQr/4xS/U2toacV9CQkLYbcuyIsaudnXNtepvNM+6detUVVVl3+7t7VV2draKi4uVlpZ2w+e+VaFQSM3NzSoqKtIDz78blTlNcNRXEtX5Pt9nh8MR1bnxO/Q5PuhzfNDn+IhHn4ffkbkVMQs6K1eu1FtvvaUPPvhAd999tz3udrslfbYjk5WVZY93d3fbuzxut1uDg4Pq6ekJ29Xp7u7WzJkz7Zpz585FPO/58+cjdouGOZ1OOZ3OiHGHwxH1F8PhcCg4dOPgdieJ1ckei9cOkehzfNDn+KDP8RHLPo9k3qhfdWVZllasWKE333xT7777rnJycsLuz8nJkdvtDtvSGhwcVEtLix1i8vPz5XA4wmq6urp09OhRu6awsFCBQEAHDx60aw4cOKBAIGDXAACAO1vUd3Sefvpp7d69W//5n/+p1NRU+/MyLpdLSUlJSkhIUGVlpaqrq5Wbm6vc3FxVV1crOTlZ5eXldu2SJUu0evVqTZo0Senp6VqzZo2mT59uX4U1bdo0zZs3T0uXLtXWrVslScuWLVNpaSlXXAEAAEkxCDovv/yyJGn27Nlh46+99pqefPJJSdLatWs1MDCg5cuXq6enRwUFBWpqalJqaqpdv3nzZo0bN04LFy7UwMCA5syZox07digxMdGu2bVrl1atWmVfnVVWVqa6urpoHxIAABijoh50LOvml1QnJCTI5/PJ5/Ndt2bChAmqra1VbW3tdWvS09NVX1//+ywTAADcAfiuKwAAYKyY/4OBgCTd+8zbUZ3PmWhp48NSnu8dnXy+NKpzAwDMwY4OAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMNW60FwB8Ufc+83ZM5v3VC4/FZF4AQPywowMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWHx7OXAdsfpWdIlvRgeAeGFHBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWFxeDoyCWF26zmXrABCOHR0AAGAsgg4AADAWb10BBon2W2LOREsbH5byfO/o5POlUZ0bAOKBHR0AAGAsgg4AADDWmA86L730knJycjRhwgTl5+frpz/96WgvCQAA3CbG9Gd03njjDVVWVuqll17SI488oq1bt2r+/Pk6fvy4pkyZMtrLA4zCt7kDGIvG9I7Opk2btGTJEv3lX/6lpk2bpi1btig7O1svv/zyaC8NAADcBsbsjs7g4KA6Ojr0zDPPhI0XFxerra3tmo8JBoMKBoP27UAgIEn65JNPFAqForKuUCik/v5+XbhwQeM+vRyVORFp3BVL/f1XNC50l4auJIz2cowVrz5fuHAhZnOPBZ//74bD4Rjt5RiLPsdHPPrc19cnSbIs66a1Yzbo/Pa3v9XQ0JAyMzPDxjMzM+X3+6/5mJqaGm3YsCFiPCcnJyZrRGyVj/YC7hDx6PPkF+PwJACM09fXJ5fLdcOaMRt0hiUkhP8t07KsiLFh69atU1VVlX37ypUr+uSTTzRp0qTrPmakent7lZ2drTNnzigtLS0qcyISfY4P+hwf9Dk+6HN8xKPPlmWpr69PHo/nprVjNuhMnjxZiYmJEbs33d3dEbs8w5xOp5xOZ9jYl770pZisLy0tjV+kOKDP8UGf44M+xwd9jo9Y9/lmOznDxuyHkcePH6/8/Hw1NzeHjTc3N2vmzJmjtCoAAHA7GbM7OpJUVVUlr9erBx98UIWFhXr11Vf18ccf66mnnhrtpQEAgNvAmA46TzzxhC5cuKDnnntOXV1dysvL0969e3XPPfeM2pqcTqeeffbZiLfIEF30OT7oc3zQ5/igz/Fxu/U5wbqVa7MAAADGoDH7GR0AAICbIegAAABjEXQAAICxCDoAAMBYBJ0oeumll5STk6MJEyYoPz9fP/3pT0d7SWOKz+dTQkJC2I/b7bbvtyxLPp9PHo9HSUlJmj17to4dOxY2RzAY1MqVKzV58mSlpKSorKxMZ8+ejfeh3FY++OADPf744/J4PEpISNCPf/zjsPuj1deenh55vV65XC65XC55vV5dvHgxxkd3+7hZn5988smI83vGjBlhNfT5xmpqavTQQw8pNTVVGRkZ+s53vqOTJ0+G1XA+f3G30uexdD4TdKLkjTfeUGVlpdavX6/Dhw/rT/7kTzR//nx9/PHHo720MeW+++5TV1eX/XPkyBH7vo0bN2rTpk2qq6vToUOH5Ha7VVRUZH+5myRVVlZqz549amhoUGtrqy5duqTS0lINDQ2NxuHcFi5fvqz7779fdXV117w/Wn0tLy9XZ2enGhsb1djYqM7OTnm93pgf3+3iZn2WpHnz5oWd33v37g27nz7fWEtLi55++mnt379fzc3N+vTTT1VcXKzLl3/3Bcqcz1/crfRZGkPns4WoePjhh62nnnoqbOxrX/ua9cwzz4zSisaeZ5991rr//vuved+VK1cst9ttvfDCC/bY//3f/1kul8t65ZVXLMuyrIsXL1oOh8NqaGiwa379619bd911l9XY2BjTtY8Vkqw9e/bYt6PV1+PHj1uSrP3799s17e3tliTrf/7nf2J8VLefq/tsWZa1ePFi69vf/vZ1H0OfR667u9uSZLW0tFiWxfkcK1f32bLG1vnMjk4UDA4OqqOjQ8XFxWHjxcXFamtrG6VVjU0ffvihPB6PcnJy9N3vflcfffSRJOnUqVPy+/1hPXY6nZo1a5bd446ODoVCobAaj8ejvLw8XofriFZf29vb5XK5VFBQYNfMmDFDLpeL3n/O+++/r4yMDH31q1/V0qVL1d3dbd9Hn0cuEAhIktLT0yVxPsfK1X0eNlbOZ4JOFPz2t7/V0NBQxJeJZmZmRnzpKK6voKBA//7v/6533nlH27Ztk9/v18yZM3XhwgW7jzfqsd/v1/jx4zVx4sTr1iBctPrq9/uVkZERMX9GRga9///mz5+vXbt26d1339WLL76oQ4cO6Vvf+paCwaAk+jxSlmWpqqpKjz76qPLy8iRxPsfCtfosja3zeUx/BcTtJiEhIey2ZVkRY7i++fPn23+ePn26CgsL9Yd/+IfauXOn/SG336fHvA43F42+Xque3v/OE088Yf85Ly9PDz74oO655x69/fbbWrBgwXUfR5+vbcWKFfrFL36h1tbWiPs4n6Pnen0eS+czOzpRMHnyZCUmJkYk0O7u7oi/WeDWpaSkaPr06frwww/tq69u1GO3263BwUH19PRctwbhotVXt9utc+fORcx//vx5en8dWVlZuueee/Thhx9Kos8jsXLlSr311lt67733dPfdd9vjnM/Rdb0+X8vtfD4TdKJg/Pjxys/PV3Nzc9h4c3OzZs6cOUqrGvuCwaBOnDihrKws5eTkyO12h/V4cHBQLS0tdo/z8/PlcDjCarq6unT06FFeh+uIVl8LCwsVCAR08OBBu+bAgQMKBAL0/jouXLigM2fOKCsrSxJ9vhWWZWnFihV688039e677yonJyfsfs7n6LhZn6/ltj6fo/ax5jtcQ0OD5XA4rO3bt1vHjx+3KisrrZSUFOtXv/rVaC9tzFi9erX1/vvvWx999JG1f/9+q7S01EpNTbV7+MILL1gul8t68803rSNHjljf+973rKysLKu3t9ee46mnnrLuvvtua9++fdbPf/5z61vf+pZ1//33W59++uloHdao6+vrsw4fPmwdPnzYkmRt2rTJOnz4sHX69GnLsqLX13nz5lnf+MY3rPb2dqu9vd2aPn26VVpaGvfjHS036nNfX5+1evVqq62tzTp16pT13nvvWYWFhdaXv/xl+jwCf/VXf2W5XC7r/ffft7q6uuyf/v5+u4bz+Yu7WZ/H2vlM0Imif/3Xf7Xuuecea/z48dYf//Efh12Kh5t74oknrKysLMvhcFgej8dasGCBdezYMfv+K1euWM8++6zldrstp9NpffOb37SOHDkSNsfAwIC1YsUKKz093UpKSrJKS0utjz/+ON6Hclt57733LEkRP4sXL7YsK3p9vXDhgrVo0SIrNTXVSk1NtRYtWmT19PTE6ShH34363N/fbxUXF1t/8Ad/YDkcDmvKlCnW4sWLI3pIn2/sWv2VZL322mt2DefzF3ezPo+18znh/x8UAACAcfiMDgAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADG+n/x3wjkoZPmdgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# longueur des reviews\n",
    "rvws_len = pd.Series(map(len, rvws_num))\n",
    "rvws_len.hist(bins=20);\n",
    "print(rvws_len.describe(percentiles=[.05, .1, .25, .5, .75, .9, .95]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# une analyse grossière de ces longueurs tend à vouloir couper les séquences entre 250 et 450\n",
    "seq_len = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction padding/truncating\n",
    "def trunc_pad(review_list, length=300):\n",
    "    res = np.zeros((len(review_list), length), dtype=int)\n",
    "    for i, rvw in enumerate(review_list):\n",
    "        res[i, -len(rvw):] = rvw[:length]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,     8,   215,    23],\n",
       "       [    0,     0,     0, ...,    29,   108,  3324],\n",
       "       [22382,    42, 46418, ...,    13,   391,    22],\n",
       "       ...,\n",
       "       [    0,     0,     0, ..., 45313,  1175,   781],\n",
       "       [    0,     0,     0, ...,    81,    95,    38],\n",
       "       [    0,     0,     0, ...,    11,     6,  1321]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc_pad(rvws_num, length=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 253 ms, sys: 4.1 ms, total: 257 ms\n",
      "Wall time: 255 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X0 = pad_sequences(rvws_num, maxlen=seq_len, padding=\"pre\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 244 ms, sys: 8.43 ms, total: 252 ms\n",
      "Wall time: 252 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = trunc_pad(rvws_num, length=seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Échantillons d'entraînement, de validation et de test**\n",
    "\n",
    "Découper les données en train, validation et test sets de la manière qui vous plaira. Tant que c'est juste et cohérent bien sûr.  \n",
    "Il faut qu'il y ait 20000 observations dans le train, 2500 dans le validation et 2500 dans le test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 250), (25000,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(lbls_num)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.52 ms, sys: 24.4 ms, total: 30.9 ms\n",
      "Wall time: 29.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# avec sklearn, en 2 fois\n",
    "X_tmp, X_test, y_tmp, y_test = train_test_split(X, y, test_size=2500)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 ms, sys: 4.21 ms, total: 19 ms\n",
      "Wall time: 17.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# à la main\n",
    "idx_shuffled = np.arange(25000)\n",
    "np.random.shuffle(idx_shuffled)\n",
    "\n",
    "X_shuffled = X[idx_shuffled]\n",
    "y_shuffled = y[idx_shuffled]\n",
    "\n",
    "nb_tr, nb_val = 20000, 2500\n",
    "\n",
    "X_train, X_val, X_test = X_shuffled[:nb_tr], X_shuffled[nb_tr:nb_tr+nb_val], X_shuffled[nb_tr+nb_val:]\n",
    "y_train, y_val, y_test = y_shuffled[:nb_tr], y_shuffled[nb_tr:nb_tr+nb_val], y_shuffled[nb_tr+nb_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 250) (2500, 250) (2500, 250)\n",
      "(20000,) (2500,) (2500,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Petite paranthèse sur le _word embedding_**\n",
    "\n",
    "Le [*word embedding*](https://fr.wikipedia.org/wiki/Word_embedding) consiste à transformer des mots sous forme de vecteurs de nombres. Bon ça on peut le faire relativement facilement, vous venez d'ailleurs de le faire avec la représentation du lexique des reviews en nombres entiers. Pour passer à un vecteur il suffirait juste de faire un one-hot-encoding.\n",
    "\n",
    "Il y a plusieurs problèmes à cette solution (même si on l'utilise parfois dans du NLP simple) :\n",
    "- la dimension de l'espace engendré\n",
    "- l'absence totale de notion de similarité (avec la représentation one-hot, 2 mots qui n'ont rien à voir sont aussi différents que 2 mots tout à fait synonyme)\n",
    "- le fait que les vecteurs contiennent quasiment que des 0 (sparse matrix)\n",
    "- le fait que les modèles sont ensuite difficilement généralisable car en cas de nouveau mot, le modèle ne sait pas du tout les traiter puisqu'il ne peut pas du tout les comparer aux mots vus dans l'entraînement\n",
    "\n",
    "Donc le *word embedding* s'attaque à ce problème avec pour objectifs de :\n",
    "1. représenter les mots sous forme de vecteurs de nombres réels (et non entiers, on passe donc à un espace continu et plus discret)\n",
    "2. conserver la notion de similarité c'est-à-dire que 2 vecteurs qui sont proches doivent représenter des mots \"sémantiquement proches\"\n",
    "3. de les représenter dans un espace de plus petite dimension (en fonction de votre vocabulaire, soit le nombre de mots dans votre problème, ça peut aller très vite, généralement plusieurs milliers ou 10aines de milliers)\n",
    "\n",
    "Le principe est de s'intéresser au contexte des mots, c'est-à-dire, quels mots sont associés ensemble en s'appuyant sur la co-occurrence.\n",
    "Différents modèles de word embedding existent : [word2vec](https://fr.wikipedia.org/wiki/Word2vec), Glove, fasttext...\n",
    "\n",
    "En pratique avec `keras`, que se passe-t-il lorsqu'on ajout une couche [*embedding*](https://keras.io/api/layers/core_layers/embedding/) ? Un exemple juste en dessous.\n",
    "\n",
    "Un peu de visionnage pour y voir plus clair si ça vous intéresse :\n",
    "- https://www.youtube.com/watch?v=Eku_pbZ3-Mw\n",
    "- https://www.youtube.com/watch?v=oUpuABKoElw\n",
    "- https://www.youtube.com/watch?v=5PL0TmQhItY\n",
    "\n",
    "Et un peu de lecture :\n",
    "- https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "- https://towardsdatascience.com/why-do-we-use-embeddings-in-nlp-2f20e1b632d2\n",
    "- https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4\n",
    "- https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n",
    "- https://medium.com/rasa-blog/supervised-word-vectors-from-scratch-in-rasa-nlu-6daf794efcd8\n",
    "- https://web.stanford.edu/~jurafsky/slp3/6.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on prend quelques phrases d'exemples pour illustrer le word embedding\n",
    "t1 = \"i hope to see you again\"\n",
    "t2 = \"you wish to see me soon\"\n",
    "t3 = \"i wish we will meet again\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'i': 2,\n",
       "          'to': 2,\n",
       "          'see': 2,\n",
       "          'you': 2,\n",
       "          'again': 2,\n",
       "          'wish': 2,\n",
       "          'hope': 1,\n",
       "          'me': 1,\n",
       "          'soon': 1,\n",
       "          'we': 1,\n",
       "          'will': 1,\n",
       "          'meet': 1}),\n",
       " {'i': 1,\n",
       "  'to': 2,\n",
       "  'see': 3,\n",
       "  'you': 4,\n",
       "  'again': 5,\n",
       "  'wish': 6,\n",
       "  'hope': 7,\n",
       "  'me': 8,\n",
       "  'soon': 9,\n",
       "  'we': 10,\n",
       "  'will': 11,\n",
       "  'meet': 12},\n",
       " 12)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# méthode mise en place au dessus\n",
    "cnts = Counter(' '.join([t1,t2,t3]).split())\n",
    "dico = {w:i+1 for i,w in enumerate(sorted(cnts, key=cnts.get, reverse=True))}\n",
    "cnts, dico, len(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 7, 2, 3, 4, 5], [4, 6, 2, 3, 8, 9], [1, 6, 10, 11, 12, 5]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# en représentant les vecteurs avec cette méthode on obtient\n",
    "t_num = [[dico[w] for w in t.split()] for t in [t1, t2, t3]]\n",
    "t_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 6, 2)              26        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26 (104.00 Byte)\n",
      "Trainable params: 26 (104.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[[[ 0.04645086  0.00891704]\n",
      "  [ 0.01984501  0.04939472]\n",
      "  [ 0.02245485 -0.04839033]\n",
      "  [-0.02122991 -0.00447623]\n",
      "  [ 0.01027304 -0.00228689]\n",
      "  [-0.01820822  0.04357253]]] [[[ 0.01027304 -0.00228689]\n",
      "  [-0.04344643  0.00133214]\n",
      "  [ 0.02245485 -0.04839033]\n",
      "  [-0.02122991 -0.00447623]\n",
      "  [-0.00037396 -0.02091723]\n",
      "  [ 0.00826902 -0.0167303 ]]] [[[ 0.04645086  0.00891704]\n",
      "  [-0.04344643  0.00133214]\n",
      "  [-0.03547596 -0.03916921]\n",
      "  [ 0.04676117  0.01137115]\n",
      "  [ 0.0404445   0.00227646]\n",
      "  [-0.01820822  0.04357253]]]\n"
     ]
    }
   ],
   "source": [
    "# on crée un modèle avec un couche embedding\n",
    "mod_emb = Sequential()\n",
    "mod_emb.add(Embedding(input_dim=13, output_dim=2, input_length=6))\n",
    "mod_emb.compile()\n",
    "print(mod_emb.summary())\n",
    "\n",
    "out1 = mod_emb.predict([t_num[0]])\n",
    "out2 = mod_emb.predict([t_num[1]])\n",
    "out3 = mod_emb.predict([t_num[2]])\n",
    "\n",
    "print(out1, out2, out3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Création, entraînement et évaluation du modèle**\n",
    "\n",
    "Créer un premier modèle, que vous serez tout à fait libre et même cordialement conviés à améliorer par la suite, avec :\n",
    "- une couche Embedding\n",
    "- une couche LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 250, 64)           4740672   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 200)               212000    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4952873 (18.89 MB)\n",
      "Trainable params: 4952873 (18.89 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Construction du modèle\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_mapping)+1,\n",
    "                    output_dim=64,\n",
    "                    input_length=seq_len))\n",
    "model.add(LSTM(units=200))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 187s 297ms/step - loss: 0.5146 - accuracy: 0.7501 - val_loss: 0.4455 - val_accuracy: 0.8068\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 210s 337ms/step - loss: 0.3058 - accuracy: 0.8789 - val_loss: 0.3545 - val_accuracy: 0.8620\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 215s 344ms/step - loss: 0.1714 - accuracy: 0.9408 - val_loss: 0.4430 - val_accuracy: 0.8456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fbcb0926050>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 7s 94ms/step - loss: 0.4661 - accuracy: 0.8372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.46605655550956726, 0.8371999859809875]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test du modèle\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.50035, 0.4888, 0.5084)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vérification de la répartition des labels\n",
    "np.mean(y), np.mean(y_train), np.mean(y_test), np.mean(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 250)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.9043721]], dtype=float32), 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prédiction d'une review\n",
    "review_test = rvws[idx_shuffled[22502]]\n",
    "x_rvw_test = np.expand_dims(X[idx_shuffled[22502]], axis=0)\n",
    "y_rvw_test = y[idx_shuffled[22502]]\n",
    "pred = model.predict(x_rvw_test)\n",
    "pred, y_rvw_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 7s 94ms/step\n"
     ]
    }
   ],
   "source": [
    "# prédictions sur le test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1146,  132],\n",
       "       [ 275,  947]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrice de confusion\n",
    "y_pred = 1 * (y_pred > 0.5)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
