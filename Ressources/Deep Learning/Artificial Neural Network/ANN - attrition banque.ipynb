{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseau de neurones et attrition des clients bancaires\n",
    "\n",
    "Réseau de neurones avec Python (Keras)\n",
    "\n",
    "## La problématique\n",
    "\n",
    "On s'intéresse ici à une problème classique du domaine bancaire (mais pas que !) qui est l'attrition ou *churn* en anglais et correspond à la perte de client. \n",
    "Récemment de nombreux clients ont quitté la banque Crédit Friqué. La question est de comprendre pourquoi ces départs ? Ou plutôt de prédire s'ils vont partir...\n",
    "\n",
    "## Les données\n",
    "\n",
    "Pour répondre à cette question, la banque a sélectionné 6 mois plus tôt un sous ensemble de ses clients pour lesquels elle a stocké un certain nombre d’information puis, dans les 6 mois qui ont suivis, elle a observé si les clients avaient quitté ou non la banque. Vous voilà donc, 6 mois plus tard, contacté par la banque qui vous propose un beau dataset (pour une fois!) et vous demande de déterminer les profils des clients les plus à même de partir.\n",
    "Vous disposez du fichier banque_abandon.csv qui est la base de données de la banque virtuelle Crédit Friqué.\n",
    "\n",
    "## Quelques questions préliminaires\n",
    "\n",
    "C'est juste pour vous échauffer donc ça doit être fait en moins d'une heure ça !\n",
    "1. À quoi correspondent les différentes variables du datasets ?\n",
    "2. Pour pas perdre les bonnes habitudes, faites quelques visualisations pour voir ce qu'il y a dans vos données.\n",
    "3. À quelle type de problème avez-vous à faire ici : classification ou régression ?\n",
    ">- Lister un certain nombre de modèles vous permettant de le résoudre\n",
    ">- Lister les métriques associées à ce type de problème\n",
    ">- Choisir un modèle, l'entraîner et l'évaluer avec la métrique de votre choix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset\n",
    "df = pd.read_csv('data/banque_abandon.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RowNumber</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>5.000500e+03</td>\n",
       "      <td>2886.895680</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2500.75</td>\n",
       "      <td>5.000500e+03</td>\n",
       "      <td>7.500250e+03</td>\n",
       "      <td>10000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CustomerId</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>1.569094e+07</td>\n",
       "      <td>71936.186123</td>\n",
       "      <td>15565701.00</td>\n",
       "      <td>15628528.25</td>\n",
       "      <td>1.569074e+07</td>\n",
       "      <td>1.575323e+07</td>\n",
       "      <td>15815690.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CreditScore</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>6.505288e+02</td>\n",
       "      <td>96.653299</td>\n",
       "      <td>350.00</td>\n",
       "      <td>584.00</td>\n",
       "      <td>6.520000e+02</td>\n",
       "      <td>7.180000e+02</td>\n",
       "      <td>850.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>3.892180e+01</td>\n",
       "      <td>10.487806</td>\n",
       "      <td>18.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>3.700000e+01</td>\n",
       "      <td>4.400000e+01</td>\n",
       "      <td>92.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tenure</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>5.012800e+00</td>\n",
       "      <td>2.892174</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balance</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>7.648589e+04</td>\n",
       "      <td>62397.405202</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.719854e+04</td>\n",
       "      <td>1.276442e+05</td>\n",
       "      <td>250898.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumOfProducts</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>1.530200e+00</td>\n",
       "      <td>0.581654</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HasCrCard</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>7.055000e-01</td>\n",
       "      <td>0.455840</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsActiveMember</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>5.151000e-01</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>1.000902e+05</td>\n",
       "      <td>57510.492818</td>\n",
       "      <td>11.58</td>\n",
       "      <td>51002.11</td>\n",
       "      <td>1.001939e+05</td>\n",
       "      <td>1.493882e+05</td>\n",
       "      <td>199992.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exited</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>2.037000e-01</td>\n",
       "      <td>0.402769</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   count          mean           std          min  \\\n",
       "RowNumber        10000.0  5.000500e+03   2886.895680         1.00   \n",
       "CustomerId       10000.0  1.569094e+07  71936.186123  15565701.00   \n",
       "CreditScore      10000.0  6.505288e+02     96.653299       350.00   \n",
       "Age              10000.0  3.892180e+01     10.487806        18.00   \n",
       "Tenure           10000.0  5.012800e+00      2.892174         0.00   \n",
       "Balance          10000.0  7.648589e+04  62397.405202         0.00   \n",
       "NumOfProducts    10000.0  1.530200e+00      0.581654         1.00   \n",
       "HasCrCard        10000.0  7.055000e-01      0.455840         0.00   \n",
       "IsActiveMember   10000.0  5.151000e-01      0.499797         0.00   \n",
       "EstimatedSalary  10000.0  1.000902e+05  57510.492818        11.58   \n",
       "Exited           10000.0  2.037000e-01      0.402769         0.00   \n",
       "\n",
       "                         25%           50%           75%          max  \n",
       "RowNumber            2500.75  5.000500e+03  7.500250e+03     10000.00  \n",
       "CustomerId       15628528.25  1.569074e+07  1.575323e+07  15815690.00  \n",
       "CreditScore           584.00  6.520000e+02  7.180000e+02       850.00  \n",
       "Age                    32.00  3.700000e+01  4.400000e+01        92.00  \n",
       "Tenure                  3.00  5.000000e+00  7.000000e+00        10.00  \n",
       "Balance                 0.00  9.719854e+04  1.276442e+05    250898.09  \n",
       "NumOfProducts           1.00  1.000000e+00  2.000000e+00         4.00  \n",
       "HasCrCard               0.00  1.000000e+00  1.000000e+00         1.00  \n",
       "IsActiveMember          0.00  1.000000e+00  1.000000e+00         1.00  \n",
       "EstimatedSalary     51002.11  1.001939e+05  1.493882e+05    199992.48  \n",
       "Exited                  0.00  0.000000e+00  0.000000e+00         1.00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           10000 non-null  int64  \n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df==0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2037"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df.Exited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dans le vif du sujet\n",
    "\n",
    "Vous l'aurez compris, il s'agit ici de résoudre le problème à l'aide d'un réseau de neurones.   \n",
    "Vous aurez bien sûr besoin du package `keras` et il vous faudra aussi certainement installer `tensorflow`(et peut-être `theano` si besoin).  \n",
    "À vous de jouer !\n",
    "N'oubliez pas le preprocessing !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 10), (10000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# création de X et y\n",
    "X = df.iloc[:, 3:-1]\n",
    "y = df.iloc[:, -1]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 10), (2000, 10), (8000,), (2000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# découpage train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y, random_state=0)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0          619    France  Female   42       2       0.00              1   \n",
       "1          608     Spain  Female   41       1   83807.86              1   \n",
       "2          502    France  Female   42       8  159660.80              3   \n",
       "3          699    France  Female   39       1       0.00              2   \n",
       "4          850     Spain  Female   43       2  125510.82              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  \n",
       "0          1               1        101348.88  \n",
       "1          0               1        112542.58  \n",
       "2          1               0        113931.57  \n",
       "3          0               0         93826.63  \n",
       "4          1               1         79084.10  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Geography\n",
       "France     5014\n",
       "Germany    2509\n",
       "Spain      2477\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.value_counts('Geography')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 11), (2000, 11))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline de preprocessing\n",
    "# onehotencoder pour les variables catégoriques\n",
    "# standard scaler pour les variables numériques\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (OneHotEncoder(drop='first'), ['Geography', 'Gender']),\n",
    "    (StandardScaler(), make_column_selector(dtype_exclude='object'))\n",
    ")\n",
    "\n",
    "X_train_pp = preprocessor.fit_transform(X_train)\n",
    "X_test_pp = preprocessor.transform(X_test)\n",
    "X_train_pp.shape, X_test_pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 12:13:07.270911: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 12:13:08.637348: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 2ms/step - loss: 0.5234 - accuracy: 0.7629 - val_loss: 0.4619 - val_accuracy: 0.8000\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4394 - accuracy: 0.8040 - val_loss: 0.4245 - val_accuracy: 0.8060\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.4042 - accuracy: 0.8232 - val_loss: 0.3911 - val_accuracy: 0.8360\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3760 - accuracy: 0.8451 - val_loss: 0.3638 - val_accuracy: 0.8520\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3569 - accuracy: 0.8543 - val_loss: 0.3518 - val_accuracy: 0.8520\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3466 - accuracy: 0.8565 - val_loss: 0.3498 - val_accuracy: 0.8495\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3423 - accuracy: 0.8581 - val_loss: 0.3449 - val_accuracy: 0.8515\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3397 - accuracy: 0.8612 - val_loss: 0.3439 - val_accuracy: 0.8535\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3371 - accuracy: 0.8618 - val_loss: 0.3466 - val_accuracy: 0.8525\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3358 - accuracy: 0.8620 - val_loss: 0.3418 - val_accuracy: 0.8540\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3343 - accuracy: 0.8634 - val_loss: 0.3418 - val_accuracy: 0.8565\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3332 - accuracy: 0.8631 - val_loss: 0.3416 - val_accuracy: 0.8545\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3322 - accuracy: 0.8641 - val_loss: 0.3419 - val_accuracy: 0.8575\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3311 - accuracy: 0.8645 - val_loss: 0.3428 - val_accuracy: 0.8525\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3305 - accuracy: 0.8651 - val_loss: 0.3431 - val_accuracy: 0.8545\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3300 - accuracy: 0.8634 - val_loss: 0.3417 - val_accuracy: 0.8560\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3295 - accuracy: 0.8658 - val_loss: 0.3424 - val_accuracy: 0.8555\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3289 - accuracy: 0.8659 - val_loss: 0.3425 - val_accuracy: 0.8520\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3285 - accuracy: 0.8651 - val_loss: 0.3414 - val_accuracy: 0.8530\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3278 - accuracy: 0.8651 - val_loss: 0.3451 - val_accuracy: 0.8520\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3283 - accuracy: 0.8649 - val_loss: 0.3416 - val_accuracy: 0.8530\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3272 - accuracy: 0.8637 - val_loss: 0.3413 - val_accuracy: 0.8560\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3270 - accuracy: 0.8626 - val_loss: 0.3415 - val_accuracy: 0.8500\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3263 - accuracy: 0.8650 - val_loss: 0.3422 - val_accuracy: 0.8550\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3269 - accuracy: 0.8644 - val_loss: 0.3456 - val_accuracy: 0.8565\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3263 - accuracy: 0.8644 - val_loss: 0.3414 - val_accuracy: 0.8500\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3252 - accuracy: 0.8645 - val_loss: 0.3471 - val_accuracy: 0.8550\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3258 - accuracy: 0.8665 - val_loss: 0.3422 - val_accuracy: 0.8505\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3253 - accuracy: 0.8645 - val_loss: 0.3430 - val_accuracy: 0.8535\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3252 - accuracy: 0.8649 - val_loss: 0.3408 - val_accuracy: 0.8515\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3252 - accuracy: 0.8660 - val_loss: 0.3429 - val_accuracy: 0.8525\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3251 - accuracy: 0.8659 - val_loss: 0.3419 - val_accuracy: 0.8540\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3246 - accuracy: 0.8665 - val_loss: 0.3426 - val_accuracy: 0.8535\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3244 - accuracy: 0.8655 - val_loss: 0.3428 - val_accuracy: 0.8550\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3244 - accuracy: 0.8656 - val_loss: 0.3436 - val_accuracy: 0.8545\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3241 - accuracy: 0.8665 - val_loss: 0.3429 - val_accuracy: 0.8530\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3241 - accuracy: 0.8646 - val_loss: 0.3433 - val_accuracy: 0.8560\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3241 - accuracy: 0.8649 - val_loss: 0.3422 - val_accuracy: 0.8540\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3238 - accuracy: 0.8671 - val_loss: 0.3420 - val_accuracy: 0.8525\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3234 - accuracy: 0.8650 - val_loss: 0.3425 - val_accuracy: 0.8545\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3234 - accuracy: 0.8650 - val_loss: 0.3423 - val_accuracy: 0.8510\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3232 - accuracy: 0.8659 - val_loss: 0.3429 - val_accuracy: 0.8535\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3228 - accuracy: 0.8660 - val_loss: 0.3457 - val_accuracy: 0.8530\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3224 - accuracy: 0.8658 - val_loss: 0.3458 - val_accuracy: 0.8565\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3225 - accuracy: 0.8650 - val_loss: 0.3452 - val_accuracy: 0.8545\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3225 - accuracy: 0.8669 - val_loss: 0.3430 - val_accuracy: 0.8545\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3221 - accuracy: 0.8683 - val_loss: 0.3454 - val_accuracy: 0.8520\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3228 - accuracy: 0.8662 - val_loss: 0.3450 - val_accuracy: 0.8555\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3221 - accuracy: 0.8655 - val_loss: 0.3441 - val_accuracy: 0.8545\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3224 - accuracy: 0.8652 - val_loss: 0.3431 - val_accuracy: 0.8520\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3225 - accuracy: 0.8660 - val_loss: 0.3431 - val_accuracy: 0.8525\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8668 - val_loss: 0.3447 - val_accuracy: 0.8525\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3221 - accuracy: 0.8651 - val_loss: 0.3478 - val_accuracy: 0.8540\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3223 - accuracy: 0.8654 - val_loss: 0.3469 - val_accuracy: 0.8520\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3221 - accuracy: 0.8661 - val_loss: 0.3431 - val_accuracy: 0.8530\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3220 - accuracy: 0.8654 - val_loss: 0.3438 - val_accuracy: 0.8535\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8658 - val_loss: 0.3460 - val_accuracy: 0.8555\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3218 - accuracy: 0.8668 - val_loss: 0.3448 - val_accuracy: 0.8525\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3214 - accuracy: 0.8684 - val_loss: 0.3443 - val_accuracy: 0.8500\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3211 - accuracy: 0.8668 - val_loss: 0.3461 - val_accuracy: 0.8550\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3211 - accuracy: 0.8664 - val_loss: 0.3450 - val_accuracy: 0.8520\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3217 - accuracy: 0.8651 - val_loss: 0.3430 - val_accuracy: 0.8525\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3212 - accuracy: 0.8664 - val_loss: 0.3448 - val_accuracy: 0.8535\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3210 - accuracy: 0.8681 - val_loss: 0.3460 - val_accuracy: 0.8545\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.8662 - val_loss: 0.3454 - val_accuracy: 0.8490\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3212 - accuracy: 0.8673 - val_loss: 0.3463 - val_accuracy: 0.8545\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3209 - accuracy: 0.8658 - val_loss: 0.3455 - val_accuracy: 0.8530\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3199 - accuracy: 0.8661 - val_loss: 0.3476 - val_accuracy: 0.8510\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3210 - accuracy: 0.8669 - val_loss: 0.3449 - val_accuracy: 0.8545\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3208 - accuracy: 0.8654 - val_loss: 0.3448 - val_accuracy: 0.8515\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3207 - accuracy: 0.8669 - val_loss: 0.3463 - val_accuracy: 0.8505\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3200 - accuracy: 0.8649 - val_loss: 0.3467 - val_accuracy: 0.8525\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3203 - accuracy: 0.8686 - val_loss: 0.3442 - val_accuracy: 0.8520\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3206 - accuracy: 0.8680 - val_loss: 0.3454 - val_accuracy: 0.8520\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3205 - accuracy: 0.8673 - val_loss: 0.3455 - val_accuracy: 0.8515\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3203 - accuracy: 0.8659 - val_loss: 0.3471 - val_accuracy: 0.8540\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3198 - accuracy: 0.8666 - val_loss: 0.3445 - val_accuracy: 0.8540\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3199 - accuracy: 0.8658 - val_loss: 0.3449 - val_accuracy: 0.8565\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3187 - accuracy: 0.8661 - val_loss: 0.3471 - val_accuracy: 0.8500\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3196 - accuracy: 0.8668 - val_loss: 0.3493 - val_accuracy: 0.8530\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3194 - accuracy: 0.8655 - val_loss: 0.3456 - val_accuracy: 0.8540\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3191 - accuracy: 0.8655 - val_loss: 0.3462 - val_accuracy: 0.8540\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3188 - accuracy: 0.8650 - val_loss: 0.3454 - val_accuracy: 0.8515\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3199 - accuracy: 0.8627 - val_loss: 0.3465 - val_accuracy: 0.8530\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3190 - accuracy: 0.8671 - val_loss: 0.3496 - val_accuracy: 0.8520\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3192 - accuracy: 0.8652 - val_loss: 0.3507 - val_accuracy: 0.8480\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3188 - accuracy: 0.8662 - val_loss: 0.3485 - val_accuracy: 0.8530\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3187 - accuracy: 0.8690 - val_loss: 0.3481 - val_accuracy: 0.8510\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3184 - accuracy: 0.8670 - val_loss: 0.3465 - val_accuracy: 0.8560\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3183 - accuracy: 0.8639 - val_loss: 0.3524 - val_accuracy: 0.8525\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3185 - accuracy: 0.8655 - val_loss: 0.3460 - val_accuracy: 0.8495\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3188 - accuracy: 0.8661 - val_loss: 0.3481 - val_accuracy: 0.8545\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3181 - accuracy: 0.8685 - val_loss: 0.3475 - val_accuracy: 0.8525\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3188 - accuracy: 0.8676 - val_loss: 0.3468 - val_accuracy: 0.8510\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3181 - accuracy: 0.8679 - val_loss: 0.3484 - val_accuracy: 0.8535\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3186 - accuracy: 0.8658 - val_loss: 0.3457 - val_accuracy: 0.8490\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3185 - accuracy: 0.8665 - val_loss: 0.3484 - val_accuracy: 0.8515\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3174 - accuracy: 0.8662 - val_loss: 0.3538 - val_accuracy: 0.8495\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3180 - accuracy: 0.8683 - val_loss: 0.3521 - val_accuracy: 0.8525\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3181 - accuracy: 0.8665 - val_loss: 0.3502 - val_accuracy: 0.8505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0ea87bc580>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on passe à l'ANN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# architecture du réseau\n",
    "mlp = Sequential()\n",
    "mlp.add(Dense(input_dim=11, units=12, activation='relu'))\n",
    "mlp.add(Dense(units=8, activation='relu'))\n",
    "mlp.add(Dense(units=6, activation='relu'))\n",
    "mlp.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# hyperparamètres d'apprentissage\n",
    "mlp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# entraînement\n",
    "mlp.fit(X_train_pp, y_train, batch_size=32, epochs=100, validation_data=(X_test_pp, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12)                144       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 54        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 309\n",
      "Trainable params: 309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3502 - accuracy: 0.8505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3502129912376404, 0.8504999876022339]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.evaluate(X_test_pp, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = 1 * (mlp.predict(X_test_pp) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cbar=False, cmap='Blues', fmt='d');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91      1593\n",
      "           1       0.75      0.40      0.52       407\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.81      0.68      0.72      2000\n",
      "weighted avg       0.84      0.85      0.83      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amélioration du modèle via resampling\n",
    "\n",
    "Les résultats n'étant pas très probants, on peut essayer d'améliorer le modèle via des téchniques de rééchantillonage pour compenser le fait que les classes soient un peu déséquilibrée (ce n'est pas très marqué dans ce cas précis mais ça reste utile de le voir).\n",
    "\n",
    "Une technique majoritairement utilisée est SMOTE qui consiste à générer et ajouter des observations dans la classe minoritaire.\n",
    "\n",
    "Éventuellement, si le déséquilibre des classes est très important, on peut combiner le sur-échantillonage (*oversampling*) de la classe minoritaire avec un sous-échantillonage aléatoire (*random undersampling*) de la classe majoritaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A FAIRE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation du réseau et affinage des hyper-paramètres\n",
    "\n",
    "Jusqu'à maintenant, on a évalué les réseaux qu'on a vu en regardant uniquement l'accuracy mais cette valeur n'est pas déterministe puisqu'elle dépend de certains paramètres aléatoires comme le train_test_split, l'intialisation des paramètres etc...\n",
    "\n",
    "Une solution par rapport à ce problème est de répéter l'entraînement plusieurs fois et de regarder les résultats en moyenne. On l'a déjà utilisé et ça s'appelle la validation croisée.\n",
    "\n",
    "Mettez en place la validation croisée en utilisant `cross_val_score` puis affiner les paramètres avec `GridSearchCV`.\n",
    "\n",
    "__/!\\\\__ Vous aurez besoin de ce qu'on appelle un wrapper pour pouvoir relier `keras` à `sklearn` et utiliser un modèle de l'un dans l'autre. Ça tombe bien, ça existe : regarder la librairie `scikeras.wrappers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition d'une fonction pour instancier le modèle\n",
    "def build_ann_clf(optimizer='adam'):\n",
    "    clf = Sequential()\n",
    "    clf.add(Dense(units=8, activation='relu', input_dim=11))\n",
    "    clf.add(Dense(units=6, activation='relu'))\n",
    "    clf.add(Dense(units=1, activation='sigmoid'))\n",
    "    clf.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 12:19:01.918837: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-17 12:19:01.942419: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-17 12:19:01.954852: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-17 12:19:02.020369: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-17 12:19:02.082607: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-17 12:19:04.324053: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-17 12:19:04.510888: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 12:19:04.594865: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 12:19:05.004426: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-17 12:19:05.058487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.4973 - accuracy: 0.7823\n",
      "Epoch 2/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.5894 - accuracy: 0.6950\n",
      "Epoch 2/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.5046 - accuracy: 0.7770\n",
      "Epoch 2/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.5374 - accuracy: 0.7177\n",
      "Epoch 2/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.5076 - accuracy: 0.7811\n",
      "Epoch 2/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.4295 - accuracy: 0.8008\n",
      "Epoch 3/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.4243 - accuracy: 0.8228\n",
      "Epoch 3/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.4228 - accuracy: 0.8142\n",
      "Epoch 3/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.4264 - accuracy: 0.8205\n",
      "338/640 [==============>...............] - ETA: 0s - loss: 0.4101 - accuracy: 0.8089Epoch 3/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.4427 - accuracy: 0.7977\n",
      "319/640 [=============>................] - ETA: 0s - loss: 0.3955 - accuracy: 0.8348Epoch 3/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.4054 - accuracy: 0.8128\n",
      "Epoch 4/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3967 - accuracy: 0.8358\n",
      "Epoch 4/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.4019 - accuracy: 0.8294\n",
      "Epoch 4/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.4052 - accuracy: 0.8291\n",
      "Epoch 4/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.4271 - accuracy: 0.8056\n",
      "Epoch 4/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3824 - accuracy: 0.8405\n",
      "Epoch 5/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3783 - accuracy: 0.8456\n",
      "Epoch 5/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3839 - accuracy: 0.8373\n",
      "Epoch 5/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3888 - accuracy: 0.8352\n",
      "Epoch 5/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.4167 - accuracy: 0.8133\n",
      "Epoch 5/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3621 - accuracy: 0.8498\n",
      "Epoch 6/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3628 - accuracy: 0.8561\n",
      "Epoch 6/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3697 - accuracy: 0.8458\n",
      "Epoch 6/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3722 - accuracy: 0.8473\n",
      "Epoch 6/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.4055 - accuracy: 0.8175\n",
      "Epoch 6/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3506 - accuracy: 0.8558\n",
      "580/640 [==========================>...] - ETA: 0s - loss: 0.3556 - accuracy: 0.8560Epoch 7/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3538 - accuracy: 0.8575\n",
      "Epoch 7/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3604 - accuracy: 0.8534\n",
      "Epoch 7/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3610 - accuracy: 0.8544\n",
      "Epoch 7/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3914 - accuracy: 0.8258\n",
      "Epoch 7/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3446 - accuracy: 0.8583\n",
      "Epoch 8/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3484 - accuracy: 0.8608\n",
      "Epoch 8/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3540 - accuracy: 0.8561\n",
      "Epoch 8/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3536 - accuracy: 0.8581\n",
      "Epoch 8/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3733 - accuracy: 0.8428\n",
      "Epoch 8/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3412 - accuracy: 0.8589\n",
      "Epoch 9/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3453 - accuracy: 0.8612\n",
      "Epoch 9/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3510 - accuracy: 0.8589\n",
      "Epoch 9/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3499 - accuracy: 0.8569\n",
      "Epoch 9/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3591 - accuracy: 0.8506\n",
      "Epoch 9/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3385 - accuracy: 0.8600\n",
      "Epoch 10/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3434 - accuracy: 0.8600\n",
      "Epoch 10/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3495 - accuracy: 0.8573\n",
      "127/640 [====>.........................] - ETA: 1s - loss: 0.3546 - accuracy: 0.8583Epoch 10/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3468 - accuracy: 0.8597\n",
      "Epoch 10/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3505 - accuracy: 0.8544\n",
      "Epoch 10/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3361 - accuracy: 0.8592\n",
      "Epoch 11/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3419 - accuracy: 0.8606\n",
      "Epoch 11/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3474 - accuracy: 0.8587\n",
      "Epoch 11/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3452 - accuracy: 0.8598\n",
      "Epoch 11/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3457 - accuracy: 0.8591\n",
      " 75/640 [==>...........................] - ETA: 1s - loss: 0.3298 - accuracy: 0.8680Epoch 11/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3357 - accuracy: 0.8567\n",
      "Epoch 12/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3406 - accuracy: 0.8623\n",
      "Epoch 12/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3465 - accuracy: 0.8586\n",
      "Epoch 12/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3442 - accuracy: 0.8570\n",
      "Epoch 12/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3422 - accuracy: 0.8614\n",
      "Epoch 12/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3349 - accuracy: 0.8612\n",
      "Epoch 13/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3398 - accuracy: 0.8631\n",
      "Epoch 13/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3464 - accuracy: 0.8584\n",
      "Epoch 13/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3425 - accuracy: 0.8594\n",
      "Epoch 13/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3388 - accuracy: 0.8631\n",
      "Epoch 13/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3334 - accuracy: 0.8611\n",
      "Epoch 14/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3388 - accuracy: 0.8627\n",
      "Epoch 14/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3451 - accuracy: 0.8594\n",
      "217/640 [=========>....................] - ETA: 0s - loss: 0.3186 - accuracy: 0.8696Epoch 14/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3410 - accuracy: 0.8600\n",
      "Epoch 14/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3376 - accuracy: 0.8631\n",
      "Epoch 14/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3328 - accuracy: 0.8616\n",
      "Epoch 15/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3382 - accuracy: 0.8622\n",
      "Epoch 15/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3446 - accuracy: 0.8602\n",
      "Epoch 15/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3413 - accuracy: 0.8597\n",
      "Epoch 15/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3357 - accuracy: 0.8645\n",
      "Epoch 15/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3321 - accuracy: 0.8608\n",
      "Epoch 16/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3373 - accuracy: 0.8639\n",
      "365/640 [================>.............] - ETA: 0s - loss: 0.3217 - accuracy: 0.8704Epoch 16/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3438 - accuracy: 0.8606\n",
      "Epoch 16/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3396 - accuracy: 0.8616\n",
      "224/640 [=========>....................] - ETA: 0s - loss: 0.3483 - accuracy: 0.8540Epoch 16/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3342 - accuracy: 0.8636\n",
      "Epoch 16/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3319 - accuracy: 0.8619\n",
      "Epoch 17/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3370 - accuracy: 0.8605\n",
      "428/640 [===================>..........] - ETA: 0s - loss: 0.3299 - accuracy: 0.8654Epoch 17/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3433 - accuracy: 0.8612\n",
      "Epoch 17/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3391 - accuracy: 0.8591\n",
      "Epoch 17/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3333 - accuracy: 0.8642\n",
      "Epoch 17/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3315 - accuracy: 0.8625\n",
      "Epoch 18/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3367 - accuracy: 0.8652\n",
      "Epoch 18/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3430 - accuracy: 0.8594\n",
      "Epoch 18/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3383 - accuracy: 0.8616\n",
      "Epoch 18/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3333 - accuracy: 0.8630\n",
      "Epoch 18/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3313 - accuracy: 0.8612\n",
      "269/640 [===========>..................] - ETA: 0s - loss: 0.3403 - accuracy: 0.8602Epoch 19/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3361 - accuracy: 0.8639\n",
      "Epoch 19/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3424 - accuracy: 0.8597\n",
      " 78/640 [==>...........................] - ETA: 1s - loss: 0.3615 - accuracy: 0.8513Epoch 19/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3378 - accuracy: 0.8627\n",
      "Epoch 19/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3321 - accuracy: 0.8659\n",
      "Epoch 19/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3311 - accuracy: 0.8616\n",
      "Epoch 20/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3355 - accuracy: 0.8648\n",
      "Epoch 20/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3420 - accuracy: 0.8605\n",
      "Epoch 20/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3374 - accuracy: 0.8630\n",
      "Epoch 20/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3315 - accuracy: 0.8642\n",
      "Epoch 20/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3305 - accuracy: 0.8616\n",
      "Epoch 21/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3349 - accuracy: 0.8653\n",
      "Epoch 21/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3418 - accuracy: 0.8617\n",
      "199/640 [========>.....................] - ETA: 1s - loss: 0.3397 - accuracy: 0.8573Epoch 21/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3373 - accuracy: 0.8617\n",
      "Epoch 21/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3307 - accuracy: 0.8659\n",
      "Epoch 21/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3304 - accuracy: 0.8634\n",
      "Epoch 22/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3347 - accuracy: 0.8637\n",
      "Epoch 22/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3416 - accuracy: 0.8598\n",
      "Epoch 22/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3361 - accuracy: 0.8620\n",
      "Epoch 22/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3304 - accuracy: 0.8641\n",
      "Epoch 22/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3295 - accuracy: 0.8609\n",
      "Epoch 23/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3343 - accuracy: 0.8672\n",
      "Epoch 23/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3409 - accuracy: 0.8608\n",
      "Epoch 23/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3361 - accuracy: 0.8619\n",
      "Epoch 23/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3296 - accuracy: 0.8667\n",
      "Epoch 23/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3293 - accuracy: 0.8637\n",
      "Epoch 24/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3335 - accuracy: 0.8645\n",
      "372/640 [================>.............] - ETA: 0s - loss: 0.3292 - accuracy: 0.8616Epoch 24/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3409 - accuracy: 0.8616\n",
      "Epoch 24/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3295 - accuracy: 0.8623\n",
      "Epoch 24/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3359 - accuracy: 0.8611\n",
      "Epoch 24/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3291 - accuracy: 0.8644\n",
      "Epoch 25/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3331 - accuracy: 0.8647\n",
      "Epoch 25/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3403 - accuracy: 0.8627\n",
      "Epoch 25/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3291 - accuracy: 0.8655\n",
      "Epoch 25/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3353 - accuracy: 0.8627\n",
      "Epoch 25/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3288 - accuracy: 0.8645\n",
      "Epoch 26/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3326 - accuracy: 0.8659\n",
      "Epoch 26/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3399 - accuracy: 0.8623\n",
      "Epoch 26/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3287 - accuracy: 0.8650\n",
      "Epoch 26/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3348 - accuracy: 0.8620\n",
      "Epoch 26/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3290 - accuracy: 0.8627\n",
      "Epoch 27/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3329 - accuracy: 0.8633\n",
      "Epoch 27/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3397 - accuracy: 0.8628\n",
      "Epoch 27/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3283 - accuracy: 0.8666\n",
      "Epoch 27/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3351 - accuracy: 0.8630\n",
      "Epoch 27/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3282 - accuracy: 0.8644\n",
      "Epoch 28/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3325 - accuracy: 0.8677\n",
      "  1/640 [..............................] - ETA: 0s - loss: 0.4446 - accuracy: 0.8000Epoch 28/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3393 - accuracy: 0.8628\n",
      "Epoch 28/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3272 - accuracy: 0.8659\n",
      "Epoch 28/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3349 - accuracy: 0.8616\n",
      "Epoch 28/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3317 - accuracy: 0.8666\n",
      "Epoch 29/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3284 - accuracy: 0.8631\n",
      "Epoch 29/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3391 - accuracy: 0.8637\n",
      "Epoch 29/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3278 - accuracy: 0.8650\n",
      "Epoch 29/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3341 - accuracy: 0.8644\n",
      "Epoch 29/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3317 - accuracy: 0.8666\n",
      "Epoch 30/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3286 - accuracy: 0.8616\n",
      "Epoch 30/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3384 - accuracy: 0.8617\n",
      "Epoch 30/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3272 - accuracy: 0.8659\n",
      "Epoch 30/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3345 - accuracy: 0.8631\n",
      "Epoch 30/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8662\n",
      "Epoch 31/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3281 - accuracy: 0.8633\n",
      "Epoch 31/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3378 - accuracy: 0.8627\n",
      "Epoch 31/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3271 - accuracy: 0.8672\n",
      "Epoch 31/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3342 - accuracy: 0.8630\n",
      "Epoch 31/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3308 - accuracy: 0.8675\n",
      "547/640 [========================>.....] - ETA: 0s - loss: 0.3300 - accuracy: 0.8627Epoch 32/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3280 - accuracy: 0.8644\n",
      "Epoch 32/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3376 - accuracy: 0.8628\n",
      "Epoch 32/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3266 - accuracy: 0.8658\n",
      "Epoch 32/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3339 - accuracy: 0.8625\n",
      "Epoch 32/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3307 - accuracy: 0.8650\n",
      "Epoch 33/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3279 - accuracy: 0.8634\n",
      "Epoch 33/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3368 - accuracy: 0.8616\n",
      "Epoch 33/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3267 - accuracy: 0.8664\n",
      "Epoch 33/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3338 - accuracy: 0.8619\n",
      "Epoch 33/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3306 - accuracy: 0.8669\n",
      "Epoch 34/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3276 - accuracy: 0.8634\n",
      "Epoch 34/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3263 - accuracy: 0.8678\n",
      "Epoch 34/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3369 - accuracy: 0.8633\n",
      "  1/640 [..............................] - ETA: 1s - loss: 0.2717 - accuracy: 0.8000Epoch 34/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3339 - accuracy: 0.8627\n",
      "Epoch 34/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3301 - accuracy: 0.8644\n",
      "Epoch 35/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3275 - accuracy: 0.8625\n",
      "Epoch 35/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3263 - accuracy: 0.8647\n",
      "Epoch 35/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3362 - accuracy: 0.8648\n",
      "Epoch 35/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3344 - accuracy: 0.8612\n",
      "Epoch 35/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3299 - accuracy: 0.8670\n",
      "Epoch 36/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3267 - accuracy: 0.8647\n",
      "Epoch 36/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3259 - accuracy: 0.8675\n",
      "Epoch 36/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3364 - accuracy: 0.8627\n",
      "Epoch 36/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3327 - accuracy: 0.8619\n",
      "Epoch 36/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3297 - accuracy: 0.8652\n",
      "Epoch 37/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3267 - accuracy: 0.8620\n",
      "Epoch 37/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3256 - accuracy: 0.8656\n",
      "Epoch 37/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3356 - accuracy: 0.8620\n",
      "Epoch 37/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3336 - accuracy: 0.8628\n",
      "Epoch 37/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3294 - accuracy: 0.8659\n",
      "Epoch 38/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3270 - accuracy: 0.8647\n",
      "Epoch 38/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3358 - accuracy: 0.8645\n",
      "Epoch 38/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3258 - accuracy: 0.8656\n",
      "Epoch 38/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3329 - accuracy: 0.8620\n",
      "Epoch 38/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3297 - accuracy: 0.8637\n",
      "Epoch 39/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3264 - accuracy: 0.8637\n",
      "Epoch 39/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3354 - accuracy: 0.8642\n",
      "Epoch 39/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3253 - accuracy: 0.8669\n",
      "Epoch 39/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3331 - accuracy: 0.8639\n",
      "Epoch 39/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3290 - accuracy: 0.8633\n",
      "422/640 [==================>...........] - ETA: 0s - loss: 0.3266 - accuracy: 0.8664Epoch 40/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3266 - accuracy: 0.8641\n",
      "Epoch 40/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3349 - accuracy: 0.8625\n",
      "Epoch 40/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3247 - accuracy: 0.8659\n",
      "Epoch 40/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3326 - accuracy: 0.8628\n",
      "Epoch 40/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3291 - accuracy: 0.8653\n",
      "Epoch 41/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3266 - accuracy: 0.8633\n",
      "Epoch 41/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3351 - accuracy: 0.8641\n",
      "Epoch 41/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3329 - accuracy: 0.8627\n",
      "Epoch 41/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3254 - accuracy: 0.8661\n",
      "Epoch 41/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3288 - accuracy: 0.8639\n",
      "Epoch 42/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3258 - accuracy: 0.8647\n",
      "Epoch 42/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3323 - accuracy: 0.8630\n",
      "Epoch 42/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3340 - accuracy: 0.8622\n",
      "Epoch 42/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3249 - accuracy: 0.8652\n",
      "Epoch 42/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3286 - accuracy: 0.8647\n",
      "Epoch 43/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3261 - accuracy: 0.8634\n",
      "Epoch 43/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3326 - accuracy: 0.8619\n",
      "Epoch 43/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3343 - accuracy: 0.8627\n",
      "Epoch 43/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3249 - accuracy: 0.8661\n",
      "Epoch 43/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3288 - accuracy: 0.8644\n",
      "213/640 [========>.....................] - ETA: 0s - loss: 0.3333 - accuracy: 0.8592Epoch 44/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3264 - accuracy: 0.8636\n",
      "234/640 [=========>....................] - ETA: 0s - loss: 0.3340 - accuracy: 0.8637Epoch 44/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3326 - accuracy: 0.8627\n",
      "Epoch 44/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3346 - accuracy: 0.8633\n",
      "191/640 [=======>......................] - ETA: 0s - loss: 0.3255 - accuracy: 0.8623Epoch 44/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3247 - accuracy: 0.8658\n",
      "Epoch 44/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3284 - accuracy: 0.8647\n",
      "Epoch 45/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3263 - accuracy: 0.8645\n",
      "Epoch 45/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3326 - accuracy: 0.8634\n",
      "Epoch 45/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3338 - accuracy: 0.8634\n",
      "Epoch 45/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3249 - accuracy: 0.8653\n",
      "Epoch 45/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3275 - accuracy: 0.8652\n",
      "Epoch 46/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3256 - accuracy: 0.8636\n",
      "Epoch 46/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3322 - accuracy: 0.8639\n",
      "Epoch 46/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3337 - accuracy: 0.8642\n",
      "Epoch 46/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3240 - accuracy: 0.8655\n",
      "Epoch 46/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3279 - accuracy: 0.8639\n",
      "Epoch 47/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3249 - accuracy: 0.8639\n",
      "Epoch 47/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3325 - accuracy: 0.8639\n",
      "Epoch 47/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3331 - accuracy: 0.8628\n",
      "Epoch 47/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3247 - accuracy: 0.8677\n",
      "Epoch 47/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3278 - accuracy: 0.8650\n",
      "Epoch 48/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3256 - accuracy: 0.8634\n",
      "Epoch 48/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3321 - accuracy: 0.8617\n",
      "Epoch 48/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3241 - accuracy: 0.8677\n",
      "Epoch 48/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3336 - accuracy: 0.8630\n",
      "  1/640 [..............................] - ETA: 1s - loss: 0.5797 - accuracy: 0.6000Epoch 48/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3280 - accuracy: 0.8652\n",
      "Epoch 49/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3250 - accuracy: 0.8644\n",
      "Epoch 49/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3312 - accuracy: 0.8611\n",
      "Epoch 49/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3245 - accuracy: 0.8650\n",
      "Epoch 49/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3329 - accuracy: 0.8634\n",
      "Epoch 49/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3280 - accuracy: 0.8634\n",
      "416/640 [==================>...........] - ETA: 0s - loss: 0.3349 - accuracy: 0.8599Epoch 50/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3252 - accuracy: 0.8641\n",
      "247/640 [==========>...................] - ETA: 0s - loss: 0.3298 - accuracy: 0.8656Epoch 50/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3312 - accuracy: 0.8648\n",
      "494/640 [======================>.......] - ETA: 0s - loss: 0.3284 - accuracy: 0.8668Epoch 50/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3239 - accuracy: 0.8661\n",
      "Epoch 50/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3330 - accuracy: 0.8633\n",
      "Epoch 50/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3276 - accuracy: 0.8642\n",
      "Epoch 51/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3263 - accuracy: 0.8637\n",
      "Epoch 51/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3319 - accuracy: 0.8631\n",
      "Epoch 51/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3238 - accuracy: 0.8664\n",
      "Epoch 51/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3332 - accuracy: 0.8617\n",
      "Epoch 51/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3276 - accuracy: 0.8653\n",
      "Epoch 52/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3255 - accuracy: 0.8631\n",
      "Epoch 52/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3313 - accuracy: 0.8636\n",
      "Epoch 52/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3240 - accuracy: 0.8659\n",
      "Epoch 52/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3324 - accuracy: 0.8634\n",
      "Epoch 52/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3272 - accuracy: 0.8647\n",
      "Epoch 53/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3251 - accuracy: 0.8617\n",
      "Epoch 53/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3317 - accuracy: 0.8636\n",
      "565/640 [=========================>....] - ETA: 0s - loss: 0.3240 - accuracy: 0.8660Epoch 53/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3237 - accuracy: 0.8661\n",
      "Epoch 53/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3316 - accuracy: 0.8633\n",
      "Epoch 53/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3270 - accuracy: 0.8656\n",
      "202/640 [========>.....................] - ETA: 0s - loss: 0.3327 - accuracy: 0.8550Epoch 54/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3251 - accuracy: 0.8639\n",
      "Epoch 54/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3319 - accuracy: 0.8648\n",
      "Epoch 54/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3231 - accuracy: 0.8667\n",
      "Epoch 54/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3323 - accuracy: 0.8611\n",
      "593/640 [==========================>...] - ETA: 0s - loss: 0.3259 - accuracy: 0.8659Epoch 54/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3273 - accuracy: 0.8647\n",
      "Epoch 55/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3252 - accuracy: 0.8645\n",
      "Epoch 55/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3321 - accuracy: 0.8622\n",
      "Epoch 55/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3233 - accuracy: 0.8664\n",
      "Epoch 55/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3324 - accuracy: 0.8631\n",
      "Epoch 55/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3269 - accuracy: 0.8642\n",
      "Epoch 56/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3254 - accuracy: 0.8645\n",
      "Epoch 56/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8644\n",
      "Epoch 56/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3234 - accuracy: 0.8658\n",
      "Epoch 56/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3322 - accuracy: 0.8639\n",
      "Epoch 56/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3267 - accuracy: 0.8662\n",
      "Epoch 57/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3252 - accuracy: 0.8636\n",
      "Epoch 57/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3312 - accuracy: 0.8631\n",
      "Epoch 57/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3232 - accuracy: 0.8672\n",
      "Epoch 57/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3320 - accuracy: 0.8625\n",
      "Epoch 57/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3264 - accuracy: 0.8652\n",
      "Epoch 58/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3247 - accuracy: 0.8653\n",
      "Epoch 58/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3310 - accuracy: 0.8630\n",
      "Epoch 58/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3228 - accuracy: 0.8650\n",
      "Epoch 58/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3263 - accuracy: 0.8636\n",
      "Epoch 59/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3317 - accuracy: 0.8611\n",
      "Epoch 58/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3250 - accuracy: 0.8634\n",
      "Epoch 59/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3309 - accuracy: 0.8637\n",
      "Epoch 59/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3225 - accuracy: 0.8658\n",
      "Epoch 59/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3268 - accuracy: 0.8656\n",
      "Epoch 60/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8641\n",
      "237/640 [==========>...................] - ETA: 0s - loss: 0.3290 - accuracy: 0.8591Epoch 59/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3248 - accuracy: 0.8662\n",
      "Epoch 60/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3315 - accuracy: 0.8614\n",
      "Epoch 60/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3227 - accuracy: 0.8661\n",
      "Epoch 60/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3262 - accuracy: 0.8655\n",
      "Epoch 61/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3319 - accuracy: 0.8636\n",
      "Epoch 60/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3244 - accuracy: 0.8633\n",
      "Epoch 61/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3312 - accuracy: 0.8627\n",
      "Epoch 61/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3229 - accuracy: 0.8666\n",
      "Epoch 61/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3259 - accuracy: 0.8645\n",
      "Epoch 62/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3311 - accuracy: 0.8648\n",
      "Epoch 61/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3249 - accuracy: 0.8630\n",
      "Epoch 62/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3313 - accuracy: 0.8641\n",
      "585/640 [==========================>...] - ETA: 0s - loss: 0.3220 - accuracy: 0.8660Epoch 62/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3226 - accuracy: 0.8655\n",
      "383/640 [================>.............] - ETA: 0s - loss: 0.3313 - accuracy: 0.8621Epoch 62/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3260 - accuracy: 0.8656\n",
      "Epoch 63/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3311 - accuracy: 0.8609\n",
      "Epoch 62/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3241 - accuracy: 0.8641\n",
      "Epoch 63/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3311 - accuracy: 0.8620\n",
      "Epoch 63/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3223 - accuracy: 0.8669\n",
      " 97/640 [===>..........................] - ETA: 1s - loss: 0.3182 - accuracy: 0.8753Epoch 63/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3258 - accuracy: 0.8669\n",
      "Epoch 64/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3308 - accuracy: 0.8644\n",
      "Epoch 63/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3240 - accuracy: 0.8642\n",
      "Epoch 64/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8616\n",
      "Epoch 64/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3228 - accuracy: 0.8647\n",
      "Epoch 64/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3259 - accuracy: 0.8631\n",
      "441/640 [===================>..........] - ETA: 0s - loss: 0.3342 - accuracy: 0.8599Epoch 65/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8616\n",
      "Epoch 64/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3242 - accuracy: 0.8637\n",
      "Epoch 65/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3309 - accuracy: 0.8644\n",
      "Epoch 65/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3227 - accuracy: 0.8653\n",
      "Epoch 65/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3255 - accuracy: 0.8645\n",
      "Epoch 66/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3310 - accuracy: 0.8627\n",
      "Epoch 65/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3242 - accuracy: 0.8639\n",
      "Epoch 66/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3310 - accuracy: 0.8645\n",
      "Epoch 66/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3223 - accuracy: 0.8656\n",
      "Epoch 66/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3257 - accuracy: 0.8648\n",
      "Epoch 67/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3311 - accuracy: 0.8633\n",
      "Epoch 66/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3240 - accuracy: 0.8636\n",
      "542/640 [========================>.....] - ETA: 0s - loss: 0.3293 - accuracy: 0.8655Epoch 67/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8628\n",
      "Epoch 67/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3225 - accuracy: 0.8655\n",
      "Epoch 67/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3258 - accuracy: 0.8656\n",
      "Epoch 68/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3305 - accuracy: 0.8634\n",
      "335/640 [==============>...............] - ETA: 0s - loss: 0.3237 - accuracy: 0.8681Epoch 67/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3242 - accuracy: 0.8645\n",
      "Epoch 68/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3301 - accuracy: 0.8636\n",
      "Epoch 68/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3222 - accuracy: 0.8673\n",
      "Epoch 68/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3251 - accuracy: 0.8669\n",
      "Epoch 69/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3307 - accuracy: 0.8616\n",
      "Epoch 68/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3243 - accuracy: 0.8636\n",
      "Epoch 69/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3221 - accuracy: 0.8650\n",
      "515/640 [=======================>......] - ETA: 0s - loss: 0.3256 - accuracy: 0.8664Epoch 69/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3309 - accuracy: 0.8645\n",
      "323/640 [==============>...............] - ETA: 0s - loss: 0.3262 - accuracy: 0.8641Epoch 69/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3256 - accuracy: 0.8652\n",
      "446/640 [===================>..........] - ETA: 0s - loss: 0.3285 - accuracy: 0.8635Epoch 70/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3306 - accuracy: 0.8633\n",
      "Epoch 69/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3242 - accuracy: 0.8639\n",
      "Epoch 70/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3221 - accuracy: 0.8672\n",
      "Epoch 70/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3309 - accuracy: 0.8622\n",
      "Epoch 70/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3250 - accuracy: 0.8658\n",
      "Epoch 71/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3303 - accuracy: 0.8639\n",
      "Epoch 70/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3235 - accuracy: 0.8664\n",
      "Epoch 71/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3220 - accuracy: 0.8675\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3302 - accuracy: 0.8633\n",
      "Epoch 71/100\n",
      "Epoch 71/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3250 - accuracy: 0.8667\n",
      "170/640 [======>.......................] - ETA: 0s - loss: 0.3106 - accuracy: 0.8653Epoch 72/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3302 - accuracy: 0.8637\n",
      "Epoch 71/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3236 - accuracy: 0.8636\n",
      "Epoch 72/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3309 - accuracy: 0.8644\n",
      "Epoch 72/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3221 - accuracy: 0.8644\n",
      "Epoch 72/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3251 - accuracy: 0.8653\n",
      "Epoch 73/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3300 - accuracy: 0.8662\n",
      "Epoch 72/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3240 - accuracy: 0.8630\n",
      "Epoch 73/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3300 - accuracy: 0.8622\n",
      "Epoch 73/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3217 - accuracy: 0.8672\n",
      "Epoch 73/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3252 - accuracy: 0.8650\n",
      "Epoch 74/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3299 - accuracy: 0.8642\n",
      "Epoch 73/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3236 - accuracy: 0.8645\n",
      "Epoch 74/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3302 - accuracy: 0.8620\n",
      "Epoch 74/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3220 - accuracy: 0.8669\n",
      "Epoch 74/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3248 - accuracy: 0.8659\n",
      "317/640 [=============>................] - ETA: 0s - loss: 0.3315 - accuracy: 0.8650Epoch 75/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3302 - accuracy: 0.8622\n",
      "Epoch 74/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3236 - accuracy: 0.8645\n",
      "Epoch 75/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3307 - accuracy: 0.8634\n",
      "615/640 [===========================>..] - ETA: 0s - loss: 0.3240 - accuracy: 0.8659Epoch 75/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3217 - accuracy: 0.8670\n",
      "Epoch 75/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3249 - accuracy: 0.8655\n",
      "Epoch 76/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3296 - accuracy: 0.8650\n",
      "Epoch 75/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3227 - accuracy: 0.8667\n",
      "Epoch 76/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3303 - accuracy: 0.8641\n",
      "Epoch 76/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3217 - accuracy: 0.8666\n",
      "Epoch 76/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3249 - accuracy: 0.8680\n",
      " 99/640 [===>..........................] - ETA: 1s - loss: 0.3088 - accuracy: 0.8697Epoch 77/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3297 - accuracy: 0.8609\n",
      "Epoch 76/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3235 - accuracy: 0.8642\n",
      "Epoch 77/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3304 - accuracy: 0.8623\n",
      "Epoch 77/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3223 - accuracy: 0.8659\n",
      "Epoch 77/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3250 - accuracy: 0.8644\n",
      "Epoch 78/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3293 - accuracy: 0.8647\n",
      "Epoch 77/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3227 - accuracy: 0.8623\n",
      "Epoch 78/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3220 - accuracy: 0.8655\n",
      "Epoch 78/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3299 - accuracy: 0.8647\n",
      "Epoch 78/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3245 - accuracy: 0.8672\n",
      "Epoch 79/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3296 - accuracy: 0.8639\n",
      "Epoch 78/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3232 - accuracy: 0.8641\n",
      "128/640 [=====>........................] - ETA: 1s - loss: 0.3491 - accuracy: 0.8602Epoch 79/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3214 - accuracy: 0.8683\n",
      "Epoch 79/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3302 - accuracy: 0.8652\n",
      "Epoch 79/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3244 - accuracy: 0.8656\n",
      "Epoch 80/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3292 - accuracy: 0.8645\n",
      "Epoch 79/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3232 - accuracy: 0.8658\n",
      "Epoch 80/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3212 - accuracy: 0.8655\n",
      "Epoch 80/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3298 - accuracy: 0.8642\n",
      "Epoch 80/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3244 - accuracy: 0.8675\n",
      "Epoch 81/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3294 - accuracy: 0.8639\n",
      "448/640 [====================>.........] - ETA: 0s - loss: 0.3295 - accuracy: 0.8641Epoch 80/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3233 - accuracy: 0.8633\n",
      "Epoch 81/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3218 - accuracy: 0.8669\n",
      "Epoch 81/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3295 - accuracy: 0.8641\n",
      "Epoch 81/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3256 - accuracy: 0.8647\n",
      "Epoch 82/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3291 - accuracy: 0.8658\n",
      "Epoch 81/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3225 - accuracy: 0.8661\n",
      "Epoch 82/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3216 - accuracy: 0.8687\n",
      "Epoch 82/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3296 - accuracy: 0.8644\n",
      "Epoch 82/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3246 - accuracy: 0.8667\n",
      "Epoch 83/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3289 - accuracy: 0.8653\n",
      "Epoch 82/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3226 - accuracy: 0.8652\n",
      "Epoch 83/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3300 - accuracy: 0.8641\n",
      "Epoch 83/100\n",
      "640/640 [==============================] - 2s 4ms/step - loss: 0.3216 - accuracy: 0.8664\n",
      "147/640 [=====>........................] - ETA: 1s - loss: 0.3255 - accuracy: 0.8578Epoch 83/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3246 - accuracy: 0.8680\n",
      "Epoch 84/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3287 - accuracy: 0.8625\n",
      "Epoch 83/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3224 - accuracy: 0.8653\n",
      "Epoch 84/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3216 - accuracy: 0.8653\n",
      "Epoch 84/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3242 - accuracy: 0.8659\n",
      " 21/640 [..............................] - ETA: 1s - loss: 0.3482 - accuracy: 0.8714Epoch 85/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3299 - accuracy: 0.8636\n",
      "Epoch 84/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3283 - accuracy: 0.8637\n",
      "Epoch 84/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3226 - accuracy: 0.8647\n",
      "Epoch 85/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3251 - accuracy: 0.8639\n",
      " 73/640 [==>...........................] - ETA: 1s - loss: 0.3166 - accuracy: 0.8726Epoch 86/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3213 - accuracy: 0.8667\n",
      "Epoch 85/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3290 - accuracy: 0.8633\n",
      "Epoch 85/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3286 - accuracy: 0.8636\n",
      "Epoch 85/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3228 - accuracy: 0.8641\n",
      "167/640 [======>.......................] - ETA: 1s - loss: 0.3371 - accuracy: 0.8563Epoch 86/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3212 - accuracy: 0.8641\n",
      "Epoch 86/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3247 - accuracy: 0.8673\n",
      " 48/640 [=>............................] - ETA: 1s - loss: 0.3044 - accuracy: 0.8813Epoch 87/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3303 - accuracy: 0.8622\n",
      "Epoch 86/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3284 - accuracy: 0.8645\n",
      "Epoch 86/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3212 - accuracy: 0.8666\n",
      "Epoch 87/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3210 - accuracy: 0.8658\n",
      " 17/640 [..............................] - ETA: 2s - loss: 0.3576 - accuracy: 0.8882Epoch 87/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3243 - accuracy: 0.8673\n",
      "Epoch 88/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3304 - accuracy: 0.8636\n",
      "Epoch 87/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3285 - accuracy: 0.8641\n",
      "Epoch 87/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3209 - accuracy: 0.8653\n",
      "Epoch 88/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3246 - accuracy: 0.8661\n",
      "Epoch 89/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3227 - accuracy: 0.8655\n",
      "Epoch 88/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3300 - accuracy: 0.8639\n",
      "Epoch 88/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3278 - accuracy: 0.8645\n",
      "Epoch 88/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3245 - accuracy: 0.8645\n",
      "Epoch 90/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3213 - accuracy: 0.8658\n",
      "Epoch 89/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3219 - accuracy: 0.8653\n",
      "Epoch 89/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3301 - accuracy: 0.8630\n",
      "Epoch 89/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3278 - accuracy: 0.8645\n",
      "Epoch 89/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3241 - accuracy: 0.8666\n",
      "Epoch 91/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3223 - accuracy: 0.8636\n",
      "Epoch 90/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3208 - accuracy: 0.8666\n",
      "121/640 [====>.........................] - ETA: 1s - loss: 0.3321 - accuracy: 0.8537Epoch 90/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3304 - accuracy: 0.8653\n",
      "140/640 [=====>........................] - ETA: 0s - loss: 0.3207 - accuracy: 0.8721Epoch 90/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3280 - accuracy: 0.8642\n",
      "Epoch 90/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3246 - accuracy: 0.8678\n",
      "Epoch 92/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3220 - accuracy: 0.8666\n",
      "Epoch 91/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3208 - accuracy: 0.8669\n",
      "Epoch 91/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3302 - accuracy: 0.8652\n",
      "173/640 [=======>......................] - ETA: 0s - loss: 0.3420 - accuracy: 0.8566Epoch 91/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3271 - accuracy: 0.8645\n",
      "Epoch 91/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3243 - accuracy: 0.8658\n",
      "Epoch 93/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3221 - accuracy: 0.8639\n",
      "Epoch 92/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3211 - accuracy: 0.8655\n",
      "Epoch 92/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3296 - accuracy: 0.8625\n",
      "Epoch 92/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3271 - accuracy: 0.8628\n",
      "Epoch 92/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3245 - accuracy: 0.8659\n",
      "Epoch 94/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3221 - accuracy: 0.8623\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3210 - accuracy: 0.8683\n",
      "  1/640 [..............................] - ETA: 2s - loss: 0.3450 - accuracy: 0.9000Epoch 93/100\n",
      "Epoch 93/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3290 - accuracy: 0.8650\n",
      " 92/640 [===>..........................] - ETA: 1s - loss: 0.3266 - accuracy: 0.8620Epoch 93/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3276 - accuracy: 0.8662\n",
      "Epoch 93/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3211 - accuracy: 0.8645\n",
      "632/640 [============================>.] - ETA: 0s - loss: 0.3243 - accuracy: 0.8674Epoch 94/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3236 - accuracy: 0.8675\n",
      "Epoch 95/100\n",
      "640/640 [==============================] - 2s 2ms/step - loss: 0.3210 - accuracy: 0.8644\n",
      "Epoch 94/100\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3300 - accuracy: 0.8634\n",
      "Epoch 94/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3275 - accuracy: 0.8653\n",
      "Epoch 94/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3242 - accuracy: 0.8683\n",
      "Epoch 96/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3213 - accuracy: 0.8642\n",
      "Epoch 95/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3214 - accuracy: 0.8658\n",
      "Epoch 95/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3297 - accuracy: 0.8628\n",
      "Epoch 95/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3267 - accuracy: 0.8627\n",
      "Epoch 95/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3239 - accuracy: 0.8664\n",
      "Epoch 97/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3208 - accuracy: 0.8652\n",
      " 44/640 [=>............................] - ETA: 1s - loss: 0.3156 - accuracy: 0.8591Epoch 96/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3207 - accuracy: 0.8650\n",
      "Epoch 96/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3295 - accuracy: 0.8655\n",
      "Epoch 96/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3241 - accuracy: 0.8666\n",
      "578/640 [==========================>...] - ETA: 0s - loss: 0.3200 - accuracy: 0.8649Epoch 98/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3271 - accuracy: 0.8661\n",
      "Epoch 96/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3211 - accuracy: 0.8650\n",
      "Epoch 97/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3213 - accuracy: 0.8652\n",
      "Epoch 97/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3293 - accuracy: 0.8645\n",
      "Epoch 97/100\n",
      "640/640 [==============================] - 2s 4ms/step - loss: 0.3240 - accuracy: 0.8673\n",
      "Epoch 99/100\n",
      "640/640 [==============================] - 2s 4ms/step - loss: 0.3274 - accuracy: 0.8634\n",
      "536/640 [========================>.....] - ETA: 0s - loss: 0.3263 - accuracy: 0.8696Epoch 97/100\n",
      "640/640 [==============================] - 2s 4ms/step - loss: 0.3209 - accuracy: 0.8667\n",
      "Epoch 98/100\n",
      "640/640 [==============================] - 2s 4ms/step - loss: 0.3212 - accuracy: 0.8659\n",
      "Epoch 98/100\n",
      "640/640 [==============================] - 2s 4ms/step - loss: 0.3296 - accuracy: 0.8659\n",
      "Epoch 98/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3230 - accuracy: 0.8680\n",
      "Epoch 100/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3201 - accuracy: 0.8664\n",
      "Epoch 99/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3263 - accuracy: 0.8653\n",
      "Epoch 98/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3209 - accuracy: 0.8655\n",
      "Epoch 99/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3298 - accuracy: 0.8656\n",
      "Epoch 99/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3242 - accuracy: 0.8656\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3207 - accuracy: 0.8681\n",
      "Epoch 100/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3266 - accuracy: 0.8652\n",
      "Epoch 99/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3211 - accuracy: 0.8648\n",
      "Epoch 100/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3291 - accuracy: 0.8633\n",
      "Epoch 100/100\n",
      "160/160 [==============================] - 0s 2ms/steposs: 0.3699 - accuracy: 0.85\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3205 - accuracy: 0.8655\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3267 - accuracy: 0.8645\n",
      "Epoch 100/100\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3208 - accuracy: 0.8637\n",
      "640/640 [==============================] - 2s 3ms/step - loss: 0.3300 - accuracy: 0.8644\n",
      "160/160 [==============================] - 0s 2ms/steposs: 0.3089 - accuracy: 0.87\n",
      "160/160 [==============================] - 0s 2ms/steposs: 0.3254 - accuracy: 0.86\n",
      "160/160 [==============================] - 0s 2ms/steposs: 0.3339 - accuracy: 0.86\n",
      "640/640 [==============================] - 1s 2ms/step - loss: 0.3266 - accuracy: 0.8669\n",
      "160/160 [==============================] - 0s 947us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.85875 , 0.8625  , 0.859375, 0.864375, 0.856875])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# création d'un wrapper pour utiliser le modèle keras dans sklearn\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = KerasClassifier(model=build_ann_clf, batch_size=10, epochs=100)\n",
    "acc = cross_val_score(estimator=clf, X=X_train_pp, y=y_train, cv=5, n_jobs=-1)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.860375, 0.002698379143115356)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.mean(), acc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affinage des hyperparamètres par validation croisée : GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = KerasClassifier(model=build_ann_clf)\n",
    "\n",
    "parametres = {\n",
    "    'batch_size' : [16, 32],\n",
    "    'epochs': [50, 120],\n",
    "    'optimizer' : ['adam', 'rmsprop']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=clf, param_grid=parametres, scoring='accuracy', cv=5)\n",
    "grid = grid.fit(X_train_pp, y_train, verbose=0)\n",
    "grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_.score(X_test_pp, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde et chargement des réseaux\n",
    "\n",
    "Regarder les méthodes `save` et `load_model` de la librairie `keras.models` pour la sauvegarde et le chargement des modèle. Quel format de fichier utiliser ?\n",
    "\n",
    "Si vous souhaitez ne sauvegarder que l'architecture du modèle (sans les poids ni la configuration d'entraînement), vous pouvez utiliser `to_json`.\n",
    "\n",
    "Enfin, pour ne sauvegarder que les poids, vous avez la méthode `save_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12)                144       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 54        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 309\n",
      "Trainable params: 309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.save()\n",
    "\n",
    "Cette fonction sauvegarde : \n",
    "- l'architecture du modèle : permet de le recréer si besoin\n",
    "- les poids du modèle\n",
    "- les paramètres d'apprentissage (loss, optimizer, metrics de l'étape `compile`)\n",
    "- l'état de l'optimisation ce qui permet de reprendre l'apprentissage où on l'avait laissé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.save('models/mlp_bank.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.load_model()\n",
    "charge un modèle enregistrés et l'ensemble des infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "new_model = load_model('models/mlp_bank.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12)                144       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 54        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 309\n",
      "Trainable params: 309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.07338906,  0.11071418,  0.4464077 , -0.06093422, -0.0696731 ,\n",
       "         0.7932484 ,  0.09280672,  0.32271478, -0.29248407,  0.08179784,\n",
       "         0.21503699,  0.3060662 ], dtype=float32),\n",
       " array([-0.07338906,  0.11071418,  0.4464077 , -0.06093422, -0.0696731 ,\n",
       "         0.7932484 ,  0.09280672,  0.32271478, -0.29248407,  0.08179784,\n",
       "         0.21503699,  0.3060662 ], dtype=float32))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.get_weights()[1], mlp.get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<keras.optimizers.optimizer_experimental.adam.Adam at 0x7f2eff20e9e0>,\n",
       " <function keras.losses.binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1)>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.optimizer, new_model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 919us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.06786656],\n",
       "       [0.11973999],\n",
       "       [0.02038068],\n",
       "       ...,\n",
       "       [0.00522138],\n",
       "       [0.9996634 ],\n",
       "       [0.07408586]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.predict(X_test_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3550 - accuracy: 0.8520\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3468 - accuracy: 0.8550\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3425 - accuracy: 0.8580\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3412 - accuracy: 0.8635\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3371 - accuracy: 0.8610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2e84367d00>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.fit(X_test_pp, y_test, batch_size=20, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.to_json()\n",
    "\n",
    "Si on a juste besoin de sauvegarder **la structure d'un réseau, sans ses paramètres d'apprentissage ni ses poids**, on peut utiliser les 2 fonctions suivantes (pour sauvegarder et charger)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 11], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"dense_input\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"batch_input_shape\": [null, 11], \"units\": 12, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 8, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 6, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_3\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}]}, \"keras_version\": \"2.11.0\", \"backend\": \"tensorflow\"}'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sauvegarde en json\n",
    "json = mlp.to_json()\n",
    "json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12)                144       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 54        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 309\n",
      "Trainable params: 309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# reconstruire un modèle depuis une sauvegarde json\n",
    "from keras.models import model_from_json\n",
    "new_model2 = model_from_json(json)\n",
    "new_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.40831864, -0.33541036,  0.27498072, -0.05016643,  0.0675357 ,\n",
       "         -0.39825007,  0.33475518, -0.37613082,  0.17338061, -0.08147529,\n",
       "         -0.12247854,  0.06741709],\n",
       "        [ 0.4212711 , -0.47033143,  0.28409368, -0.26040912,  0.16047448,\n",
       "         -0.1624853 , -0.20982268, -0.06730345,  0.45654136, -0.37401417,\n",
       "          0.1853782 , -0.43690687],\n",
       "        [ 0.20376569,  0.25068712, -0.040371  , -0.47779772,  0.12304646,\n",
       "          0.30698228,  0.36637497,  0.14083183, -0.07420179, -0.00253677,\n",
       "         -0.07684207, -0.3207592 ],\n",
       "        [-0.18534249, -0.5073993 , -0.4779179 , -0.44750637,  0.06012273,\n",
       "         -0.05046758,  0.04971004,  0.07557684, -0.11172816, -0.1169588 ,\n",
       "         -0.4031009 ,  0.0463078 ],\n",
       "        [ 0.25004548, -0.30518854,  0.35143107,  0.13259655,  0.03291678,\n",
       "          0.2780878 , -0.20101923, -0.41877106,  0.12047547,  0.25284237,\n",
       "          0.3403631 ,  0.22994608],\n",
       "        [ 0.3042062 , -0.42715272, -0.3808061 , -0.2298617 , -0.44004864,\n",
       "          0.12312114,  0.12164879, -0.09900469, -0.04307449, -0.19974595,\n",
       "          0.32148862, -0.37650943],\n",
       "        [ 0.08059609,  0.49452502, -0.17238596, -0.01908198, -0.11463562,\n",
       "         -0.38219684,  0.27824777, -0.1648337 ,  0.47592932, -0.37273675,\n",
       "         -0.23011377, -0.05974889],\n",
       "        [-0.14732745, -0.49063748, -0.26085457, -0.15461779,  0.18519175,\n",
       "          0.37668085, -0.28646153,  0.5064294 ,  0.41967148, -0.12217727,\n",
       "          0.26018637,  0.4343326 ],\n",
       "        [ 0.434614  ,  0.15361995,  0.41331905, -0.3431387 ,  0.22604388,\n",
       "          0.3718992 ,  0.24699163, -0.46505487,  0.03745973, -0.11682937,\n",
       "          0.39382422,  0.27336055],\n",
       "        [-0.46252173,  0.48916978, -0.30718976, -0.24260673,  0.07988691,\n",
       "         -0.14479166,  0.02038509,  0.37497628,  0.27159083,  0.15664458,\n",
       "          0.0897969 ,  0.0080933 ],\n",
       "        [ 0.15803218,  0.43458903, -0.08677244, -0.06768072, -0.45418757,\n",
       "          0.31314218, -0.21610716, -0.20829955, -0.23577416,  0.32395536,\n",
       "         -0.2502984 ,  0.04147464]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.3958013 , -0.52208465, -0.33769757, -0.04365546, -0.32732046,\n",
       "          0.4074061 ,  0.28177488, -0.48564884],\n",
       "        [ 0.11116058, -0.5468154 ,  0.15130305,  0.42987984, -0.3461191 ,\n",
       "         -0.4950647 , -0.4252017 ,  0.0528211 ],\n",
       "        [-0.28105286, -0.36339268,  0.46286154,  0.3193844 , -0.13475338,\n",
       "         -0.4641338 , -0.03832018, -0.51730424],\n",
       "        [-0.48142737,  0.26058602, -0.3862461 ,  0.4808383 , -0.19524896,\n",
       "          0.4492607 , -0.34165618, -0.17207468],\n",
       "        [-0.26083404,  0.15492791,  0.457505  ,  0.1185292 , -0.2276462 ,\n",
       "          0.44274718, -0.0775837 ,  0.32952213],\n",
       "        [ 0.5153309 , -0.41498756,  0.13754845,  0.3491289 , -0.30952847,\n",
       "         -0.12082714,  0.07357883, -0.03391445],\n",
       "        [-0.2061646 , -0.2865002 , -0.10665399,  0.30977553,  0.00952321,\n",
       "          0.44554394,  0.34560943, -0.00653577],\n",
       "        [ 0.4811257 ,  0.09195012,  0.5341538 , -0.18296304,  0.17974418,\n",
       "         -0.08148277, -0.05378014,  0.5362506 ],\n",
       "        [ 0.01114404, -0.16843611, -0.5052799 ,  0.4999162 ,  0.4715693 ,\n",
       "         -0.5165608 , -0.41806838,  0.46731865],\n",
       "        [-0.25799116, -0.5144542 , -0.44976062,  0.40894765,  0.14143485,\n",
       "         -0.29016   ,  0.5303993 , -0.54091597],\n",
       "        [-0.234142  , -0.09867826,  0.4713422 , -0.12107369, -0.36501682,\n",
       "          0.31434453,  0.28190118, -0.4926712 ],\n",
       "        [-0.54170644,  0.4821043 ,  0.30479455,  0.02693951, -0.40157312,\n",
       "          0.30532014, -0.23443869, -0.11646971]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.4956535 ,  0.45232952, -0.3023193 , -0.59698856,  0.20119542,\n",
       "         -0.2763921 ],\n",
       "        [-0.6152295 ,  0.2979192 ,  0.14159876,  0.54414713,  0.56611   ,\n",
       "          0.55001926],\n",
       "        [-0.09131587, -0.15757063, -0.43043417,  0.03095007, -0.19915512,\n",
       "         -0.4787185 ],\n",
       "        [ 0.15226352, -0.04818863,  0.03641927,  0.09154606,  0.5696949 ,\n",
       "          0.4121977 ],\n",
       "        [-0.46346214,  0.24835563,  0.21473002, -0.576132  , -0.5302679 ,\n",
       "         -0.44087964],\n",
       "        [-0.5952008 ,  0.16808861, -0.2667319 , -0.30302837,  0.02037221,\n",
       "         -0.51969135],\n",
       "        [ 0.6349157 ,  0.6347157 ,  0.34766936,  0.17536503, -0.29322815,\n",
       "          0.14976484],\n",
       "        [-0.22756758,  0.02546859, -0.3289209 ,  0.6503093 ,  0.6350044 ,\n",
       "         -0.07445419]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[-0.03669816],\n",
       "        [ 0.454597  ],\n",
       "        [ 0.20492983],\n",
       "        [-0.490284  ],\n",
       "        [-0.1221804 ],\n",
       "        [ 0.07635605]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model2.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(new_model2.optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.save_weights()\n",
    "\n",
    "Si jamais, on veut uniquement les poids d'un modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.save_weights('models/mes_poids.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_7 (Dense)             (None, 12)                144       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 6)                 54        \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 309\n",
      "Trainable params: 309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model3 = Sequential([\n",
    "    Dense(input_dim=11, units=12, activation='relu'),\n",
    "    Dense(units=8, activation='relu'),\n",
    "    Dense(units=6, activation='relu'),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "new_model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.21559072,  0.28623104, -0.48189685, -0.16205496,  0.10648251,\n",
       "          0.24715275,  0.11330402, -0.33103502, -0.23792893,  0.16062194,\n",
       "          0.2889105 , -0.23972252],\n",
       "        [-0.30371678,  0.07563663,  0.10232103, -0.0028882 , -0.41555554,\n",
       "         -0.49630445,  0.08060986, -0.49669984, -0.23907408, -0.01577583,\n",
       "         -0.29307526,  0.16051   ],\n",
       "        [-0.25841215, -0.4872366 , -0.20125377,  0.22911012, -0.3018936 ,\n",
       "          0.50161165,  0.11215341,  0.44376975,  0.07085437,  0.17110455,\n",
       "         -0.21066451, -0.04468638],\n",
       "        [ 0.2094379 ,  0.42682564, -0.38961673, -0.2610869 , -0.21924257,\n",
       "         -0.05233958,  0.38973302,  0.28877157,  0.0582422 , -0.44053596,\n",
       "         -0.06123292, -0.15980336],\n",
       "        [ 0.42279947,  0.15829557,  0.3546241 , -0.13185227, -0.01681677,\n",
       "         -0.0137516 ,  0.03943223,  0.11025727,  0.2914878 , -0.39598888,\n",
       "         -0.40718532,  0.36133438],\n",
       "        [ 0.0119406 , -0.19339281,  0.34712458, -0.21793717, -0.43495375,\n",
       "         -0.14091659,  0.3160919 , -0.4944149 , -0.22876683,  0.44582045,\n",
       "          0.19200873, -0.23427197],\n",
       "        [-0.394625  ,  0.29856503, -0.10490033,  0.1354683 , -0.18028137,\n",
       "          0.40951812,  0.18929297, -0.03434199, -0.4454618 ,  0.08015883,\n",
       "         -0.20421785,  0.236803  ],\n",
       "        [ 0.23810077,  0.30064946, -0.14681712,  0.1699472 ,  0.18828672,\n",
       "          0.4922238 ,  0.4940638 ,  0.38665664, -0.06093192,  0.15896761,\n",
       "          0.10077184, -0.32477272],\n",
       "        [-0.23500687, -0.4235867 ,  0.45513862, -0.15737545, -0.48253298,\n",
       "          0.14311826, -0.37483686,  0.03432751, -0.04132167, -0.5022687 ,\n",
       "          0.22157598,  0.16638082],\n",
       "        [ 0.4673146 , -0.33458716, -0.48598695,  0.3106295 , -0.05339706,\n",
       "         -0.09577039,  0.11761111,  0.29212254,  0.48770666, -0.38859445,\n",
       "          0.36422443, -0.02570924],\n",
       "        [ 0.3577841 , -0.44706106, -0.0393028 , -0.07102448, -0.21659476,\n",
       "         -0.33570394,  0.45913583, -0.08744159,  0.05927968, -0.01411584,\n",
       "         -0.187819  ,  0.50291103]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[-0.17218423, -0.40793157,  0.07054973, -0.25441712, -0.30236626,\n",
       "          0.230578  , -0.50121486, -0.42653555],\n",
       "        [-0.21136484, -0.5195549 , -0.13588202,  0.50098395,  0.00834388,\n",
       "          0.32498723,  0.47495723,  0.06148428],\n",
       "        [-0.06928945,  0.30752796,  0.26109195,  0.33862448,  0.35129493,\n",
       "         -0.07152107,  0.05537617, -0.17700133],\n",
       "        [-0.33578444, -0.1850476 ,  0.51687145, -0.16042003, -0.25915873,\n",
       "         -0.18226269,  0.14277637,  0.21408051],\n",
       "        [-0.50357705,  0.02411526,  0.28012645,  0.08104193,  0.0984385 ,\n",
       "          0.03144187,  0.15811306,  0.5087031 ],\n",
       "        [-0.28461763,  0.3336072 ,  0.2245971 , -0.20071128,  0.34781623,\n",
       "          0.47540665, -0.5372503 ,  0.32291025],\n",
       "        [-0.15313351, -0.04599178,  0.22467655, -0.34862185, -0.32315916,\n",
       "          0.28735358,  0.14504743, -0.45749006],\n",
       "        [-0.35940313,  0.154118  ,  0.0623771 , -0.38064182,  0.0374583 ,\n",
       "          0.13530642,  0.09390122, -0.2011767 ],\n",
       "        [-0.40253007,  0.32703733,  0.1830231 ,  0.01645607,  0.30682385,\n",
       "         -0.30829334, -0.29499656,  0.49527884],\n",
       "        [ 0.13130915,  0.4421761 , -0.46337393,  0.01677626, -0.3412267 ,\n",
       "         -0.31712013, -0.07555464, -0.00324249],\n",
       "        [ 0.23386008, -0.41558933, -0.13094336,  0.1646586 , -0.1792374 ,\n",
       "         -0.21619946, -0.5164993 ,  0.5429021 ],\n",
       "        [-0.2803208 , -0.33265454,  0.24043214,  0.45303845, -0.23101705,\n",
       "         -0.15074378, -0.52161217,  0.1723178 ]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.49745107,  0.04394007,  0.0894171 ,  0.00347733,  0.58544505,\n",
       "         -0.08584332],\n",
       "        [ 0.03626496,  0.11412609, -0.26658112,  0.4040221 ,  0.44915628,\n",
       "         -0.08272046],\n",
       "        [-0.02134323, -0.21125442, -0.23535857,  0.25570893, -0.5261074 ,\n",
       "         -0.24301624],\n",
       "        [ 0.17355728,  0.32482904,  0.5489098 , -0.33995914, -0.40144312,\n",
       "          0.5140941 ],\n",
       "        [-0.6493491 , -0.06349462, -0.34200802,  0.13797033,  0.17349517,\n",
       "         -0.18127066],\n",
       "        [-0.39331642, -0.19250247,  0.4317627 , -0.55496526,  0.49290144,\n",
       "          0.01902825],\n",
       "        [-0.48815173, -0.4166475 ,  0.01429504,  0.30385166, -0.1158545 ,\n",
       "         -0.48798382],\n",
       "        [ 0.17239308,  0.2805969 , -0.04237914, -0.5222859 , -0.31189147,\n",
       "          0.17260766]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.80975294],\n",
       "        [ 0.08802867],\n",
       "        [-0.2818483 ],\n",
       "        [ 0.8168353 ],\n",
       "        [ 0.02154398],\n",
       "        [-0.3550219 ]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# poids initialisés par défaut\n",
    "new_model3.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.31149316,  0.8738772 , -0.6099201 , -0.3927553 , -0.12982355,\n",
       "         -0.61505383, -0.31023297, -0.37443832, -0.1117999 ,  0.20785624,\n",
       "          0.4542791 , -0.76075333],\n",
       "        [-0.39752898,  0.26805022,  0.49453327,  0.40669817, -0.6826332 ,\n",
       "         -0.07212142,  0.08525822,  0.06844214, -0.15129475, -0.02902561,\n",
       "         -0.4032772 ,  0.2210217 ],\n",
       "        [-0.05515415, -0.09383014,  0.5438555 ,  0.18435252, -0.12511644,\n",
       "          0.49554932, -0.03893202,  0.6045929 , -0.00198024, -0.02248016,\n",
       "         -0.33551267,  0.02430101],\n",
       "        [ 0.04092321, -0.04377514,  0.04927093,  0.00352005,  0.32075462,\n",
       "         -0.19835487, -0.04912436,  0.24773583,  0.08215123, -0.13014889,\n",
       "          0.1328827 , -0.28860998],\n",
       "        [ 1.0607488 ,  0.0471989 , -0.38643596,  0.7445689 ,  0.7315348 ,\n",
       "         -0.3051959 ,  0.10334988, -0.84376186, -0.45660028,  0.4123941 ,\n",
       "         -0.28595525, -0.15927798],\n",
       "        [-0.01904048, -0.26059812, -0.22366692, -0.1421348 , -0.10483622,\n",
       "          0.3586826 , -0.11845129, -0.11637564, -0.17696077,  0.0326937 ,\n",
       "         -0.4925921 ,  0.10511763],\n",
       "        [-0.22998719, -0.16096024,  0.04374468,  0.5007157 ,  0.3257276 ,\n",
       "          0.67114735,  0.5236551 ,  0.04303723, -0.48453337, -0.04969785,\n",
       "          0.24600847, -0.14939435],\n",
       "        [ 0.29398212, -0.6202929 , -0.07493746,  0.40810186, -0.9696681 ,\n",
       "         -0.7199863 ,  0.836332  ,  0.06109207,  0.9127727 ,  0.887199  ,\n",
       "         -0.11306234, -0.75983703],\n",
       "        [-0.15160137, -0.08218788, -0.44824538,  0.12412721, -0.16935475,\n",
       "          0.02841303, -0.08539355,  0.46474835,  0.02938992, -0.01797492,\n",
       "         -0.29933915, -0.03726517],\n",
       "        [ 0.0751648 , -0.44054714,  0.43651155,  0.39144492, -0.00983486,\n",
       "         -0.3441006 , -0.22024508, -0.25086716, -0.0720687 ,  0.0571466 ,\n",
       "         -0.47100842, -0.29433882],\n",
       "        [ 0.06614508,  0.02143152, -0.20916201,  0.3513974 ,  0.06378022,\n",
       "         -0.26819602,  0.01432291,  0.33226034, -0.42216295, -0.04684098,\n",
       "          0.1870696 ,  0.5247952 ]], dtype=float32),\n",
       " array([-0.07338906,  0.11071418,  0.4464077 , -0.06093422, -0.0696731 ,\n",
       "         0.7932484 ,  0.09280672,  0.32271478, -0.29248407,  0.08179784,\n",
       "         0.21503699,  0.3060662 ], dtype=float32),\n",
       " array([[ 5.36500692e-01, -3.86108041e-01,  1.27743214e-01,\n",
       "          3.06514531e-01, -4.80754197e-01, -5.91693163e-01,\n",
       "         -3.36430669e-01,  8.39198411e-01],\n",
       "        [-3.04602891e-01,  3.20922107e-01,  6.19428277e-01,\n",
       "         -5.80282062e-02, -4.77377623e-01, -2.60384411e-01,\n",
       "          9.44227353e-03,  5.39029479e-01],\n",
       "        [ 1.46566451e-01,  5.81326008e-01, -1.31074727e-01,\n",
       "         -1.54405519e-01,  5.25263548e-02,  7.94773176e-02,\n",
       "          4.28413957e-01, -1.92095280e-01],\n",
       "        [ 7.20614612e-01,  3.15261245e-01,  2.88900256e-01,\n",
       "          1.72692001e-01,  3.39194626e-01,  1.58793792e-01,\n",
       "          7.60662719e-04, -3.48335683e-01],\n",
       "        [ 5.17531820e-02,  5.11811674e-01,  5.05354226e-01,\n",
       "         -7.19077945e-01, -4.95038390e-01,  2.88349867e-01,\n",
       "         -1.35492921e-01,  5.49761169e-02],\n",
       "        [-2.93159515e-01, -4.98012491e-02,  5.52569628e-01,\n",
       "         -8.33621740e-01,  6.61314651e-02,  4.12482977e-01,\n",
       "          2.10014582e-01,  1.44156829e-01],\n",
       "        [-5.29978991e-01,  5.14388084e-01, -4.86838520e-01,\n",
       "          8.01159024e-01, -5.82159102e-01, -4.43872511e-01,\n",
       "         -6.88156426e-01,  1.07759953e+00],\n",
       "        [ 6.13241076e-01,  9.80348811e-02,  9.31845456e-02,\n",
       "         -2.45775744e-01,  3.17221820e-01,  2.87686735e-01,\n",
       "          1.15244485e-01, -1.86037183e-01],\n",
       "        [-1.67081594e-01,  1.01684153e-01, -5.47829628e-01,\n",
       "          3.79342437e-01,  3.28663260e-01, -5.30352771e-01,\n",
       "          8.46830904e-01, -8.74208733e-02],\n",
       "        [ 3.03277135e-01, -6.20227993e-01, -1.72069490e-01,\n",
       "          3.05426538e-01,  5.10108173e-01,  4.33787197e-01,\n",
       "          5.94705641e-01,  2.13757649e-01],\n",
       "        [-6.68337792e-02,  3.96439195e-01,  1.93008870e-01,\n",
       "         -8.14596891e-01,  8.07267904e-01,  5.97026289e-01,\n",
       "          6.32539392e-01,  3.90736014e-03],\n",
       "        [-6.10987663e-01, -5.02369881e-01,  4.16169524e-01,\n",
       "         -1.18696839e-01, -1.37558877e-01, -4.38907072e-02,\n",
       "          8.47687498e-02, -5.12544960e-02]], dtype=float32),\n",
       " array([ 0.02667397,  0.06439003,  0.14735878, -0.11196841, -0.27868897,\n",
       "         0.50715   ,  0.41140723,  0.595595  ], dtype=float32),\n",
       " array([[-0.3432968 , -0.89594436,  0.81538665, -0.18836696,  0.8643891 ,\n",
       "          0.8632208 ],\n",
       "        [-0.29853714, -0.73459184, -0.9185097 ,  0.6623641 ,  0.3818709 ,\n",
       "         -0.61045116],\n",
       "        [-0.40730324, -0.31810045,  0.29438835,  0.48811877, -0.39002383,\n",
       "         -0.24823906],\n",
       "        [-0.4347002 ,  0.04555455, -0.2176099 ,  0.8494006 , -0.8466417 ,\n",
       "         -0.64583063],\n",
       "        [-0.55354077, -0.19142985,  0.10679587,  0.3455853 , -0.42198616,\n",
       "         -0.21898955],\n",
       "        [ 0.67123723, -0.48391274,  0.09578708,  0.13721488,  0.7273199 ,\n",
       "          0.55847305],\n",
       "        [ 0.549513  ,  0.61461073,  0.0692438 ,  0.10373398,  0.6574946 ,\n",
       "          0.47543514],\n",
       "        [ 0.26484352,  0.11972236, -0.15870853,  0.6685415 ,  0.24348316,\n",
       "          0.09959526]], dtype=float32),\n",
       " array([ 0.37773153,  0.3333    , -0.35140172,  0.02378836,  0.10340258,\n",
       "         0.21772282], dtype=float32),\n",
       " array([[-0.80180764],\n",
       "        [-1.1711128 ],\n",
       "        [-0.81509024],\n",
       "        [ 0.85943025],\n",
       "        [-1.0805224 ],\n",
       "        [-0.49611685]], dtype=float32),\n",
       " array([0.03175563], dtype=float32)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chargement des poids sauvegardés\n",
    "new_model3.load_weights('models/mes_poids.h5')\n",
    "new_model3.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complément sur l'overfitting\n",
    "\n",
    "Toujours sur les données de la banque, entrainer un réseau ayant une structure complexe avec beaucoup de neurones et de couches afin de générer une situation d'overfitting.  \n",
    "Comparer l'accuracy sur les échantillons train et test pour confirmer le cas de sur-apprentissage.\n",
    "\n",
    "Reprendre le même réseau en utilisant des layers `Dropout` pour réduire ce problème.  \n",
    "Comparer à nouveau l'accuracy pour voir l'effet des `Dropout` sur l'overfitting.\n",
    "\n",
    "Une autre méthode pour limiter le sur-apprentissage est la régularisation. Est-il possible d'en faire avec un réseau de neurones ? Si oui, allez-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_17 (Dense)            (None, 128)               1536      \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 4)                 68        \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,473\n",
      "Trainable params: 12,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_overfit = Sequential([\n",
    "    Dense(units=128, activation='relu', input_dim=11),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=16, activation='relu'),\n",
    "    Dense(units=4, activation='relu'),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "mlp_overfit.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "mlp_overfit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "500/500 [==============================] - 2s 1ms/step - loss: 0.4008 - accuracy: 0.8329\n",
      "Epoch 2/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.3498 - accuracy: 0.8540\n",
      "Epoch 3/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.3391 - accuracy: 0.8585\n",
      "Epoch 4/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.3357 - accuracy: 0.8626\n",
      "Epoch 5/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.3310 - accuracy: 0.8637\n",
      "Epoch 6/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3272 - accuracy: 0.8659\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3228 - accuracy: 0.8683\n",
      "Epoch 8/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3208 - accuracy: 0.8660\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.3149 - accuracy: 0.8711\n",
      "Epoch 10/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.3146 - accuracy: 0.8700\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.3101 - accuracy: 0.8708\n",
      "Epoch 12/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.3075 - accuracy: 0.8714\n",
      "Epoch 13/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.3021 - accuracy: 0.8742\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.2994 - accuracy: 0.8742\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.2962 - accuracy: 0.8761\n",
      "Epoch 16/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.2891 - accuracy: 0.8829\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.2852 - accuracy: 0.8826\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.2791 - accuracy: 0.8871\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.2776 - accuracy: 0.8863\n",
      "Epoch 20/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.2727 - accuracy: 0.8870\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.2651 - accuracy: 0.8903\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.2603 - accuracy: 0.8911\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.2586 - accuracy: 0.8920\n",
      "Epoch 24/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.2500 - accuracy: 0.8939\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.2466 - accuracy: 0.8985\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.2404 - accuracy: 0.8979\n",
      "Epoch 27/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.2354 - accuracy: 0.9040\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.2306 - accuracy: 0.9001\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.2227 - accuracy: 0.9080\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.2225 - accuracy: 0.9062\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.2134 - accuracy: 0.9087\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.2080 - accuracy: 0.9118\n",
      "Epoch 33/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.2040 - accuracy: 0.9154\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.1992 - accuracy: 0.9166\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.1957 - accuracy: 0.9158\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.1881 - accuracy: 0.9208\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.1802 - accuracy: 0.9234\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.1797 - accuracy: 0.9219\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1733 - accuracy: 0.9271\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1666 - accuracy: 0.9304\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1700 - accuracy: 0.9296\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1563 - accuracy: 0.9320\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1549 - accuracy: 0.9360\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1485 - accuracy: 0.9371\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.1451 - accuracy: 0.9404\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1403 - accuracy: 0.9410\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1403 - accuracy: 0.9427\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1336 - accuracy: 0.9484\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1335 - accuracy: 0.9430\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.1247 - accuracy: 0.9464\n",
      "Epoch 51/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1260 - accuracy: 0.9495\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1202 - accuracy: 0.9495\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1178 - accuracy: 0.9494\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.1165 - accuracy: 0.9509\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.1203 - accuracy: 0.9505\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1058 - accuracy: 0.9575\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1031 - accuracy: 0.9565\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1069 - accuracy: 0.9548\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.1003 - accuracy: 0.9585\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.1013 - accuracy: 0.9575\n",
      "Epoch 61/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0966 - accuracy: 0.9605\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0903 - accuracy: 0.9628\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1011 - accuracy: 0.9588\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0866 - accuracy: 0.9631\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0981 - accuracy: 0.9619\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0852 - accuracy: 0.9660\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0907 - accuracy: 0.9615\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0849 - accuracy: 0.9654\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0899 - accuracy: 0.9631\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0822 - accuracy: 0.9649\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0855 - accuracy: 0.9654\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0760 - accuracy: 0.9707\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0834 - accuracy: 0.9656\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0828 - accuracy: 0.9670\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0839 - accuracy: 0.9678\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0828 - accuracy: 0.9670\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0676 - accuracy: 0.9735\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0655 - accuracy: 0.9745\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0879 - accuracy: 0.9657\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0727 - accuracy: 0.9712\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0571 - accuracy: 0.9754\n",
      "Epoch 82/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0719 - accuracy: 0.9724\n",
      "Epoch 83/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0869 - accuracy: 0.9676\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0780 - accuracy: 0.9686\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0579 - accuracy: 0.9770\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0628 - accuracy: 0.9750\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0631 - accuracy: 0.9750\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0568 - accuracy: 0.9770\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0834 - accuracy: 0.9696\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0521 - accuracy: 0.9803\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0715 - accuracy: 0.9740\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0704 - accuracy: 0.9732\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0502 - accuracy: 0.9797\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0597 - accuracy: 0.9772\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0586 - accuracy: 0.9779\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0494 - accuracy: 0.9800\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0588 - accuracy: 0.9765\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0768 - accuracy: 0.9720\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0825 - accuracy: 0.9721\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0587 - accuracy: 0.9784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2e187e5ab0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_overfit.fit(X_train_pp, y_train, epochs=100, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0427 - accuracy: 0.9840\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4028 - accuracy: 0.8250\n",
      "[0.04272375628352165, 0.984000027179718] [1.402822494506836, 0.824999988079071]\n"
     ]
    }
   ],
   "source": [
    "print(mlp_overfit.evaluate(X_train_pp, y_train), mlp_overfit.evaluate(X_test_pp, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_23 (Dense)            (None, 128)               1536      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 4)                 68        \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,473\n",
      "Trainable params: 12,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ANN - avec DropOut\n",
    "from keras.layers import Dropout\n",
    "\n",
    "mlp_overfit_dropout = Sequential([\n",
    "    Dense(units=128, activation='relu', input_dim=11),\n",
    "    Dropout(0.3),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(units=16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(units=4, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "mlp_overfit_dropout.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "mlp_overfit_dropout.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.5055 - accuracy: 0.7900\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4506 - accuracy: 0.8061\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4203 - accuracy: 0.8207\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4081 - accuracy: 0.8269\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4031 - accuracy: 0.8345\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3966 - accuracy: 0.8376\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3889 - accuracy: 0.8378\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3856 - accuracy: 0.8428\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3858 - accuracy: 0.8420\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3786 - accuracy: 0.8446\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3787 - accuracy: 0.8456\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3697 - accuracy: 0.8479\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3695 - accuracy: 0.8471\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3749 - accuracy: 0.8432\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3754 - accuracy: 0.8446\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3716 - accuracy: 0.8435\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3745 - accuracy: 0.8450\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3645 - accuracy: 0.8490\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3700 - accuracy: 0.8476\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3689 - accuracy: 0.8449\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3635 - accuracy: 0.8520\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3642 - accuracy: 0.8501\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3648 - accuracy: 0.8487\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3645 - accuracy: 0.8478\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3597 - accuracy: 0.8526\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3594 - accuracy: 0.8529\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3599 - accuracy: 0.8547\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3603 - accuracy: 0.8496\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3568 - accuracy: 0.8515\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3592 - accuracy: 0.8533\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3572 - accuracy: 0.8544\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3556 - accuracy: 0.8518\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3555 - accuracy: 0.8529\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3579 - accuracy: 0.8537\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3569 - accuracy: 0.8506\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3492 - accuracy: 0.8559\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3599 - accuracy: 0.8521\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3543 - accuracy: 0.8533\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3543 - accuracy: 0.8541\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3544 - accuracy: 0.8559\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.3497 - accuracy: 0.8589\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3471 - accuracy: 0.8587\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3475 - accuracy: 0.8571\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3506 - accuracy: 0.8561\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3488 - accuracy: 0.8579\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3474 - accuracy: 0.8577\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3523 - accuracy: 0.8556\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3484 - accuracy: 0.8560\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3489 - accuracy: 0.8550\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3461 - accuracy: 0.8615\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3474 - accuracy: 0.8580\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3468 - accuracy: 0.8577\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3476 - accuracy: 0.8586\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3445 - accuracy: 0.8570\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3425 - accuracy: 0.8565\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3523 - accuracy: 0.8531\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3450 - accuracy: 0.8586\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3471 - accuracy: 0.8572\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3453 - accuracy: 0.8551\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3495 - accuracy: 0.8554\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3355 - accuracy: 0.8608\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3344 - accuracy: 0.8614\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3419 - accuracy: 0.8590\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3401 - accuracy: 0.8609\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3398 - accuracy: 0.8586\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3413 - accuracy: 0.8595\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3368 - accuracy: 0.8616\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3386 - accuracy: 0.8635\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3381 - accuracy: 0.8618\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3328 - accuracy: 0.8626\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3367 - accuracy: 0.8625\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3399 - accuracy: 0.8619\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3410 - accuracy: 0.8571\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3390 - accuracy: 0.8595\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3368 - accuracy: 0.8605\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3279 - accuracy: 0.8621\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3314 - accuracy: 0.8675\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3389 - accuracy: 0.8572\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3291 - accuracy: 0.8599\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3329 - accuracy: 0.8595\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3394 - accuracy: 0.8571\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3345 - accuracy: 0.8600\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3383 - accuracy: 0.8586\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3315 - accuracy: 0.8604\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3282 - accuracy: 0.8629\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3282 - accuracy: 0.8624\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3382 - accuracy: 0.8644\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3392 - accuracy: 0.8602\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3337 - accuracy: 0.8606\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3334 - accuracy: 0.8622\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3319 - accuracy: 0.8615\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3342 - accuracy: 0.8602\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3303 - accuracy: 0.8633\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3345 - accuracy: 0.8599\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3343 - accuracy: 0.8624\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3303 - accuracy: 0.8626\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3269 - accuracy: 0.8644\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3281 - accuracy: 0.8609\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3372 - accuracy: 0.8594\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3281 - accuracy: 0.8668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2e29964b80>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_overfit_dropout.fit(X_train_pp, y_train, epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 1ms/step - loss: 0.2703 - accuracy: 0.8838\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3642 - accuracy: 0.8555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.36415401101112366, 0.8554999828338623]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_overfit_dropout.evaluate(X_train_pp, y_train)\n",
    "mlp_overfit_dropout.evaluate(X_test_pp, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_29 (Dense)            (None, 128)               1536      \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 4)                 68        \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,473\n",
      "Trainable params: 12,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ANN - avec régularisation l2\n",
    "from keras import regularizers\n",
    "\n",
    "mlp_overfit_regul = Sequential([\n",
    "    Dense(units=128, activation='relu', input_dim=11),\n",
    "    Dense(units=64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),#ou kernel_regularizer='l2'\n",
    "    Dense(units=32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(units=16, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(units=4, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "mlp_overfit_regul.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "mlp_overfit_regul.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "800/800 [==============================] - 2s 1ms/step - loss: 0.6576 - accuracy: 0.8059\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4381 - accuracy: 0.8459\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4158 - accuracy: 0.8543\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4052 - accuracy: 0.8540\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3988 - accuracy: 0.8583\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3946 - accuracy: 0.8572\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3890 - accuracy: 0.8574\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3880 - accuracy: 0.8575\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3835 - accuracy: 0.8596\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3816 - accuracy: 0.8608\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3807 - accuracy: 0.8619\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3781 - accuracy: 0.8610\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3755 - accuracy: 0.8619\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3736 - accuracy: 0.8630\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3745 - accuracy: 0.8600\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3728 - accuracy: 0.8604\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3706 - accuracy: 0.8622\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3677 - accuracy: 0.8611\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3674 - accuracy: 0.8612\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3686 - accuracy: 0.8630\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3680 - accuracy: 0.8627\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3645 - accuracy: 0.8616\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3641 - accuracy: 0.8630\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3642 - accuracy: 0.8624\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3625 - accuracy: 0.8641\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3617 - accuracy: 0.8636\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3604 - accuracy: 0.8621\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3620 - accuracy: 0.8630\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3598 - accuracy: 0.8637\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3586 - accuracy: 0.8631\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3606 - accuracy: 0.8640\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3558 - accuracy: 0.8636\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3587 - accuracy: 0.8629\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3556 - accuracy: 0.8635\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3547 - accuracy: 0.8641\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3557 - accuracy: 0.8629\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3535 - accuracy: 0.8675\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3535 - accuracy: 0.8645\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3527 - accuracy: 0.8668\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3531 - accuracy: 0.8670\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3520 - accuracy: 0.8654\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3520 - accuracy: 0.8649\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3517 - accuracy: 0.8669\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3526 - accuracy: 0.8671\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3508 - accuracy: 0.8645\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3513 - accuracy: 0.8656\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3498 - accuracy: 0.8655\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3509 - accuracy: 0.8659\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3488 - accuracy: 0.8662\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3492 - accuracy: 0.8669\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3466 - accuracy: 0.8681\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3487 - accuracy: 0.8674\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3498 - accuracy: 0.8691\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3472 - accuracy: 0.8664\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3474 - accuracy: 0.8675\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3459 - accuracy: 0.8659\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3471 - accuracy: 0.8674\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3462 - accuracy: 0.8715\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3453 - accuracy: 0.8684\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3451 - accuracy: 0.8686\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3444 - accuracy: 0.8687\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3425 - accuracy: 0.8696\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3445 - accuracy: 0.8691\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3458 - accuracy: 0.8661\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3439 - accuracy: 0.8700\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3435 - accuracy: 0.8660\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3423 - accuracy: 0.8700\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3447 - accuracy: 0.8684\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3416 - accuracy: 0.8686\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3427 - accuracy: 0.8685\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3430 - accuracy: 0.8687\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3413 - accuracy: 0.8701\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3415 - accuracy: 0.8712\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3407 - accuracy: 0.8701\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3413 - accuracy: 0.8701\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3390 - accuracy: 0.8691\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3406 - accuracy: 0.8708\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3395 - accuracy: 0.8696\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3404 - accuracy: 0.8710\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3383 - accuracy: 0.8694\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3383 - accuracy: 0.8683\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3399 - accuracy: 0.8714\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3381 - accuracy: 0.8686\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3391 - accuracy: 0.8685\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3377 - accuracy: 0.8711\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3358 - accuracy: 0.8710\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3364 - accuracy: 0.8712\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3360 - accuracy: 0.8724\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3353 - accuracy: 0.8709\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3362 - accuracy: 0.8686\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3346 - accuracy: 0.8734\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3357 - accuracy: 0.8712\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3332 - accuracy: 0.8723\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3355 - accuracy: 0.8712\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3338 - accuracy: 0.8715\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3333 - accuracy: 0.8710\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3348 - accuracy: 0.8700\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.3341 - accuracy: 0.8742\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3344 - accuracy: 0.8736\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3331 - accuracy: 0.8714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2efedc9fc0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_overfit_regul.fit(X_train_pp, y_train, epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3258 - accuracy: 0.8754\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3734 - accuracy: 0.8550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.37335583567619324, 0.8550000190734863]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_overfit_regul.evaluate(X_train_pp, y_train)\n",
    "mlp_overfit_regul.evaluate(X_test_pp, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
