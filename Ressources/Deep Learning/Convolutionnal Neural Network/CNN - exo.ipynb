{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Le classique \"chien ou chat\" - CNN avec Keras**\n",
    "\n",
    "Le but de cet exercice est la classification d'images d'animaux en chien ou en chat. Évidemment ça peut paraître assez stérile comme objectif dans la vraie vie mais sachez que les applications intéressantes et utiles de ces réseaux sont multiples notamment en médecine.  \n",
    "Par exemple pour la reconnaissance de tumeur pulmonaire ou autre.  \n",
    "Et ce qui est chouette, c'est que la seule différence sera dans les images pour l'entraînement et la strutucre du réseau éventuellement.\n",
    "\n",
    "## **Les données**\n",
    "\n",
    "Les données ne sont plus structurées en un seul et joli CSV à partir duquel on peut associer le label et les variables explicatives facilement pour chaque observation. Le problème est qu'il faut pouvoir associer à chaque image le bon label.\n",
    "\n",
    "Une idée simple de solution est de structurer les données dans des dossiers (sous-dossiers) contenant les labels dans les noms par exemple :\n",
    "1. séparer les échantillons d'entraînement et de test dans 2 dossiers différents\n",
    "2. ensuite on appelle chaque image par un nom {label}_{numéro} genre chat_1, chat_2, chien_1, chien_2 etc...\n",
    "3. pour finir on fait un petit script qui récupère le label dans le nom de l'image à chaque fois via une petite manip de string\n",
    "\n",
    "On pourrait faire ça, ça marche, mais il y a une manière \"toute cousue\" qui fonctionne très bien dans Keras puisqu'on on y trouve des outils pour extraire les images facilement. La seule contrainte est de bien structurer le dossier de la manière suivante :\n",
    "\n",
    "<img src=\"images/cnn_exo_arborescence.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Le réseau CNN**\n",
    "\n",
    "Tout est dit. Ou presque.  \n",
    "Vous devez construire et entraîner un réseau de neurones à convolution pour la catégorisation des images en chien/chat.  \n",
    "Allez, je vous mets quelques pistes et même toutes les librairies nécessaires, c'est cadeau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 15:57:13.948311: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Librairies\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **L'architecture**\n",
    "\n",
    "Dans un premier temps construisez votre réseau étape par étape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 15:57:20.226523: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# 0. Initialisation\n",
    "classifier = Sequential()\n",
    "\n",
    "# 1. La Convolution\n",
    "classifier.add(Conv2D(filters=32, kernel_size=(3, 3),\n",
    "                      input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# 2. Le Max Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Ajout d'une couche de convolution/pooling supplémentaire\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# 3. Le Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# 4. Les couches fully-connected\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Paramètres d'entraînement\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               802944    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 813,217\n",
      "Trainable params: 813,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **L'entraînement**\n",
    "\n",
    "Là, c'est un peu plus complexe alors je vous donne les méthodes à utiliser, dans cet ordre :\n",
    "1. `ImageDataGenerator`\n",
    "2. `flow_from_directory`\n",
    "3. `fit`\n",
    "\n",
    "Il va certainement vous falloir fouiller un peu là-dedans https://keras.io/api/preprocessing/image/ et en profiter pour découvrir le concept merveilleux de *Data Augmentation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "250/250 [==============================] - 42s 167ms/step - loss: 0.6552 - accuracy: 0.6070 - val_loss: 0.5974 - val_accuracy: 0.6775\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 0.5907 - accuracy: 0.6839 - val_loss: 0.5424 - val_accuracy: 0.7350\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.5469 - accuracy: 0.7210 - val_loss: 0.5014 - val_accuracy: 0.7675\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.5143 - accuracy: 0.7484 - val_loss: 0.5719 - val_accuracy: 0.7255\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.5022 - accuracy: 0.7561 - val_loss: 0.5111 - val_accuracy: 0.7545\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 42s 166ms/step - loss: 0.4693 - accuracy: 0.7720 - val_loss: 0.4740 - val_accuracy: 0.7870\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 41s 163ms/step - loss: 0.4545 - accuracy: 0.7864 - val_loss: 0.4520 - val_accuracy: 0.8030\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.4426 - accuracy: 0.7943 - val_loss: 0.4625 - val_accuracy: 0.7895\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.4277 - accuracy: 0.7995 - val_loss: 0.4570 - val_accuracy: 0.7915\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.4170 - accuracy: 0.8065 - val_loss: 0.5208 - val_accuracy: 0.7680\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 0.3948 - accuracy: 0.8175 - val_loss: 0.4534 - val_accuracy: 0.7985\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 42s 166ms/step - loss: 0.3891 - accuracy: 0.8219 - val_loss: 0.4647 - val_accuracy: 0.7980\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.3724 - accuracy: 0.8334 - val_loss: 0.4926 - val_accuracy: 0.7785\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 0.3624 - accuracy: 0.8376 - val_loss: 0.4363 - val_accuracy: 0.8050\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 40s 162ms/step - loss: 0.3472 - accuracy: 0.8460 - val_loss: 0.4598 - val_accuracy: 0.8075\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 40s 158ms/step - loss: 0.3362 - accuracy: 0.8514 - val_loss: 0.4598 - val_accuracy: 0.8045\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.3134 - accuracy: 0.8639 - val_loss: 0.5002 - val_accuracy: 0.7925\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.3073 - accuracy: 0.8680 - val_loss: 0.4732 - val_accuracy: 0.8135\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.2927 - accuracy: 0.8710 - val_loss: 0.4602 - val_accuracy: 0.8150\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 42s 167ms/step - loss: 0.2843 - accuracy: 0.8794 - val_loss: 0.4840 - val_accuracy: 0.8040\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 45s 181ms/step - loss: 0.2710 - accuracy: 0.8824 - val_loss: 0.4580 - val_accuracy: 0.8190\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.2492 - accuracy: 0.8979 - val_loss: 0.5029 - val_accuracy: 0.8135\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 45s 181ms/step - loss: 0.2507 - accuracy: 0.8964 - val_loss: 0.4622 - val_accuracy: 0.8200\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 45s 180ms/step - loss: 0.2282 - accuracy: 0.9055 - val_loss: 0.5066 - val_accuracy: 0.8030\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 45s 180ms/step - loss: 0.2157 - accuracy: 0.9112 - val_loss: 0.5243 - val_accuracy: 0.8085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f52204da740>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On génère de nouvelles images de training supplémentaires avec ImageDataGenerator\n",
    "# qui au passage gère le preprocessing de l'image (notamme le rescaling)\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "classifier.fit(training_set,\n",
    "               epochs = 25,\n",
    "               validation_data = test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prédictions à partir de nouvelles images**\n",
    "\n",
    "Prenez une jolie photo de votre chien ou de votre chat et utilisez votre modèle CNN pour prédire de quel animal il s'agit. Si vous n'avez ni chien ni chat, pas de panique, dans le dossier `single_prediction`, il y a ce qu'il faut et vous pouvez aussi tester d'autres images en allant en récupérer sur internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Amélioration de votre modèle**\n",
    "\n",
    "Un petit *kaggle* maison : essayez d'améliorer l'accuracy de votre CNN\n",
    "- Accuracy **entre 80% et 85%** sur le jeu de test, c'est **pas mal**\n",
    "- Accuracy **entre 85% et 90%** sur le jeu de test, c'est **bien**\n",
    "- Accuracy **entre 90% et 95%**  sur le jeu de test, c'est **très bien**\n",
    "- Accuracy **supérieur à 95%**  sur le jeu de test, c'est **top**\n",
    "\n",
    "Les règles du jeu :\n",
    "- On garde bien sûr le même jeu d'entraînement\n",
    "- Il est interdit de spécifier manuellement une seed\n",
    "\n",
    "*Une indication utile :* laissez tomber la validation croisée dans un premier temps car en utilisant `fit_generator`, le modèle passe directement sur l'échantillon test pour valider le modèle. C'est faisable mais un peu plus compliqué..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# éventuellement ajout d'époques pour voir si on améliore encore ou si ça converge avant\n",
    "classifier.fit(training_set,\n",
    "               epochs = 40,\n",
    "               validation_data = test_set)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour aller plus loin, on peut modifier le réseau et atteindre les 90% d'accuracy espérés de différentes manières en rendant le réseau un peu plus complexe : vous pouvez notamment ajouter une couche de convolution ou augmenter le nombre de neurones dans les couches. Le problème c'est que c'est un peu long de réentraîner à chaque fois tout le modèle pour tester les améliorations.\n",
    "\n",
    "#### **Autre solution: le transfer learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Voilà à quoi ressemble le modèle VGG16 full\n",
    "from keras.applications.vgg16 import VGG16\n",
    "vgg16 = VGG16(weights='imagenet')\n",
    "print(vgg16.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
