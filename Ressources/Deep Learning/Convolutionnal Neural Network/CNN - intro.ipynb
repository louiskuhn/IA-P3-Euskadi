{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Les réseaux de neurones à convolutions**\n",
    "\n",
    "CNN = Convolution Neural Network\n",
    "\n",
    "## **L'intuition des CNN**\n",
    "\n",
    "Image > CNN > catégorisation\n",
    "\n",
    "Image découpée en pixel =\n",
    "- si N&B alors 1 tableau à 2 dimensions avec intensité du pixel (entier de 1 à 255 en niveau de gris)\n",
    "- si couleur alors 3 tableaux à 2 dimensions ou 1 tableau 3D (un tableau 2D d'intensité pour chaque couleur RGB c'est-à-dire rouge, vert et bleu)\n",
    "\n",
    "Ces tableaux sont les inputs des réseaux de neurones à convolution qui s'articulent en 4 étapes :\n",
    "1. la convolution (et la couche ReLU)\n",
    "2. Max Pooling\n",
    "3. Flattening\n",
    "4. Couche(s) entièrement connectée(s)\n",
    "\n",
    "Pour allerplus loin, les écrits saints des CNN par [Yann Lecun](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)\n",
    "\n",
    "## **1. Première étape : la convolution**\n",
    "\n",
    "### **1.a La convolution**\n",
    "La fonction de convolution (ou produit de convolution) c'est ça :\n",
    "\n",
    "$$(f*g)(t)=\\int_{-\\infty}^\\infty f(u)g(t-u)du$$\n",
    "\n",
    "C'est un peu compliqué vu comme ça, mais l'idée assez simple derrière est de voir la combinaison des 2 fonctions, où comment l'une modifie l'autre, ce qui rejoint la notion plus générale de *filtrage*. Bon on ira pas plus loin.\n",
    "\n",
    "Parce que de \"beaux\" dessins valent mieux que de longs discours, voilà comment ça marche :\n",
    "\n",
    "Passage de l'image aux tableaux numériques de pixels\n",
    "\n",
    "<img src=\"images/cnn1.png\">\n",
    "\n",
    "La convolution : on parle de *Feature Detector* ou *Kernel* ou *Filtre*. Après passage d'un filtre, on obtient une *Feature Map* (les filtres sont généralement de taille 3x3, 5x5 ou 7x7)\n",
    "\n",
    "<img src=\"images/cnn2.png\">\n",
    "\n",
    "La couche de convolution, ou convolutionnal layer ou juste convolution est un ensemble de *Feature Maps* obtenues par passage de différents filtres(*Feature detector*) sur l'image en input.\n",
    "\n",
    "Ici on a une *stride* de 1 c'est-à-dire qu'on applique le filtre en décalant de 1 pixel à chaque fois (on pourrait choisir 2 ou autre si on voulait). Quel est l'effet de la stride ? Et bien pour une stride $s$, si l'image input est de taille $n\\times p$, la *Feature map* a une taille $(n-2s)\\times (p-2s)$.\n",
    "\n",
    "Dans l'exemple ça marche bien mais parfois le couple *stride* + *Feature Detector* n'est pas exactement adapté à la taille de l'image. Il y a alors 2 options pour gérer les pixels sur lesquels le filtre n'est pas passé, c'est ce qu'on appelle le *padding* : soit on drop les pixel (*valid-padding*) soit on ajoute des zéros pour avoir la bonne dimension (*zero-padding*).\n",
    "\n",
    "<img src=\"images/cnn3.png\">\n",
    "\n",
    "Pour jouer un peu avec la convolution, installer Gimp qui est un éditeur d'image. Quand c'est fait, aller dans *Filtre>Générique>Matrice de convolution* et tester sur une image les filtres suivants (et d'autres encore !):\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$\n",
    "$\\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & -1 & 0 & 0\\\\\n",
    "0 & -1 & 5 & -1 & 0\\\\\n",
    "0 & 0 & -1 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$\n",
    "$\\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 1 & 1 & 0\\\\\n",
    "0 & 1 & 1 & 1 & 0\\\\\n",
    "0 & 1 & 1 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$\n",
    "$\\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & -1 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$\n",
    "$\\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 1 & -4 & 1 & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$\n",
    "$\\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & -2 & -1 & 0 & 0\\\\\n",
    "0 & -1 & 1 & 1 & 0\\\\\n",
    "0 & 0 & 1 & 2 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$\n",
    "$\\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0 & -1 & 0\\\\\n",
    "0 & 2 & 0 & -2 & 0\\\\\n",
    "0 & 1 & 0 & -1 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}$\n",
    "\n",
    "Ce qu'il faut retenir c'est que le but ultime de la convolution est de trouver des *features* dans une image ensuite de les enregistrer dans une *features map* qui va non seulement garder la structure de l'image et en même temps repérer où se trouvent ces éléments importants.\n",
    "\n",
    "### **1.b La couche ReLU**\n",
    "\n",
    "En images :\n",
    "\n",
    "<img src=\"images/cnn4.png\">\n",
    "\n",
    "Cette \"sous-étape\" permet de réintroduire de la non-linéarité dans une image qui a pu être \"linéarisée\" par la convolution. En pratique, on applique simplement la fonction d'activation ReLU à notre *feature map*. Pour essayer d'illustrer, même si c'est pas évident sans les maths derrière :\n",
    "\n",
    "Voilà une image quelconque\n",
    "\n",
    "<img src=\"images/cnn7.png\">\n",
    "\n",
    "On lui applique un filtre par convolution. Si on regarde au milieu de l'image il y a zone d'ombre noire verticale et en faisant attention on voit que le passage du blanc au noir autour de cette ombre, se fait de manière \"progressive\" en passant par des nuances de gris. Cette \"progressivité\", c'est de la linéarité. Donc on va la supprimer.\n",
    "\n",
    "<img src=\"images/cnn5.png\">\n",
    "\n",
    "Et pour la supprimer, on enlève les valeurs négatives (correspondant au noir) avec la fonction ReLU. De fait, au niveau de la zone d'ombre on passe plus par des nuances de gris sombres jusque au noir mais d'un coup du blanc au gris et inversément.\n",
    "\n",
    "<img src=\"images/cnn6.png\">\n",
    "\n",
    "## **2. Deuxième étape : Max Pooling**\n",
    "\n",
    "Objectif = modèle doit reconnaître toutes ces images d'un guépard pourtant très différentes (la position, l'orientation, le regard, la taille...)\n",
    "\n",
    "<img src=\"images/cnn8.png\">\n",
    "\n",
    "<img src=\"images/cnn9.png\">\n",
    "\n",
    "Par exemple, si le modèle veut identifier la \"larme noire\" au coin de l'oeil du félin, il faut que le modèle ait une propriété particulère : **l'invariance spatiale**. C'est-à-dire que le modèle doit être suffisamment souple pour pouvoir identifier qu'il s'agit du bien même feature.\n",
    "\n",
    "Pour reprendre l'exemple précédent, voilà ce qu'on donne le max pooling sur la *feature map* qu'on avait obtenue :\n",
    "\n",
    "<img src=\"images/cnn10.png\">\n",
    "\n",
    "Dans l'exemple on prend un carré de 2x2 avec une *stride* de 2 (avec un *zero-padding*).\n",
    "\n",
    "Quel est l'effet de cette étape ?\n",
    "- On a conservé les valeurs des features (ici, le 4 et les 2 notamment)\n",
    "- On a retiré de l'information (on voit bien que la dimension du problème est plus petite...) mais on retiré l'information qui **ne contenait pas les features**\n",
    "\n",
    "Par conséquent, si on 2 images un peu différentes de la même chose comme la larme du guépard, après les étapes de Convolution et de Max Pooling, on a de grandes chances d'obtenir 2 matrices similaires.  \n",
    "Au passage, un autre avantage du fait de diminuer la dimension du problème est la diminution du risque de sur-entraînement.\n",
    "\n",
    "Pourquoi le Max Pooling ? Pourquoi une *stride* de 2 ? Tout simplement parce que c'est ce qui donne les meilleurs résultats comparé à d'autres stratégies de Pooling. Si vous voulez en savoir plus, ça se passe par [là](http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf)\n",
    "\n",
    "Bon on en est là donc\n",
    "\n",
    "<img src=\"images/cnn11.png\">\n",
    "\n",
    "Ce que je vous propose c'est d'aller jouer un peu sur https://adamharley.com/nn_vis/cnn/3d.html, c'est joli.\n",
    "\n",
    "## **3. Troisième étape : Flattening**\n",
    "\n",
    "C'est ce qu'il y a de plus facile. je vous laisse deviner ce que ça fait...c'est bon vous l'avez ? Allez un petit dessin, on en est là\n",
    "\n",
    "<img src=\"images/cnn12.png\">\n",
    "\n",
    "## **4. Quatrième étape : Couche entièrement connectée**\n",
    "\n",
    "C'est la dernière étape qui consiste à utiliser les features en sortie de l'étape précédente (flattening) comme input de réseau de neurones comme on les a vus jusqu'à maintenant.\n",
    "\n",
    "<img src=\"images/cnn13.png\">\n",
    "\n",
    "Petite précision parce que ce serait trop facile sinon : les couches sont entièrement connectées, d'où le nom...et subtilité supplémentaire, au cours de la rétropropagation, on va mettre à jour non seulement les poids __mais aussi les *Feature Detector*__ pour qu'ils soient ajustés afin de minimiser l'erreur puisque c'est par eux que sont sélectionnés les features initialement.\n",
    "\n",
    "**Et voilà !** Un réseau de neurones à convolution c'est ça :\n",
    "\n",
    "<img src=\"images/cnn14.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
